{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8318545",
   "metadata": {},
   "source": [
    "<div style='width=100%; display:flex;flex-direction:row'><img  src=https://universidadeuropea.com/resources/media/images/universidad-europea-logo_poc9mEM.original.png width=100  style='  margin-left: auto;margin-right: auto; width: 25%; height:25%;'><img  src=https://i.ibb.co/1068C7j/EATEASER.jpg width=100 style='  margin-left: auto;margin-right: auto; width: 10%;height:25%;'></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ee9dce",
   "metadata": {},
   "source": [
    "<div style='margin:auto;text-align: center;font-family: \"Times New Roman\", Times, serif; font-weight: bold;'>PROYECTO COMUTACIONAL<br><br>EATEASER - VOZ A TEXTO</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c329ef59",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex;flex-direction:row'>\n",
    "    <div style='width:50%;margin-right:5cm;'>\n",
    "        <p style='font-family: \"Times New Roman\", Times, serif; font-weight: bold;'>ESTUDIANTES</p>\n",
    "<ul style='font-family: \"Times New Roman\", Times, serif;'>\n",
    "    <li>Adilem Dobras 21911633</li><li>Roberto Echevarria 21823680</li><li>Carlos Gonzales 22067726</li><li>Juan Carlos Rondeau 21816176</li></ul> </div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3dfd23",
   "metadata": {},
   "source": [
    "<h2 style=\"font-size: 19px;color:#6DA0FF;font-family:Georgia, Times, 'Times New Roman', serif;letter-spacing: 3px;font-weight: normal\">1. Importamos las librerias</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5e13fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pitu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pitu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\pitu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pathlib\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import joblib\n",
    "import warnings\n",
    "import math\n",
    "import random\n",
    "import multiprocessing\n",
    "from random import shuffle\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ModuleNotFoundError:\n",
    "    !pip install pandas\n",
    "    import pandas as pd\n",
    "try:\n",
    "    from pytube import YouTube\n",
    "    from pytube import Playlist\n",
    "except ModuleNotFoundError:\n",
    "    !pip install pytube\n",
    "    from pytube import YouTube\n",
    "    from pytube import Playlist\n",
    "try:\n",
    "    import speech_recognition as sr\n",
    "except ModuleNotFoundError:\n",
    "    !pip install SpeechRecognition\n",
    "    import speech_recognition as sr\n",
    "try:\n",
    "    from pydub import AudioSegment\n",
    "    from pydub.silence import split_on_silence\n",
    "except:\n",
    "    !pip install pydub\n",
    "    from pydub import AudioSegment\n",
    "    from pydub.silence import split_on_silence\n",
    "try:\n",
    "    import moviepy.editor as mp\n",
    "except:\n",
    "    !pip install moviepy\n",
    "    import moviepy.editor as mp\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "except:\n",
    "    !pip install beautifulsoup4\n",
    "    from bs4 import BeautifulSoup\n",
    "try:\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from nltk.tokenize import word_tokenize\n",
    "except:\n",
    "    !pip install nltk\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from nltk.tokenize import word_tokenize\n",
    "try:\n",
    "    import pyrebase\n",
    "except:\n",
    "    !pip install pyrebase4\n",
    "    import pyrebase\n",
    "try:   \n",
    "    import nltk\n",
    "    #nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.rslp import RSLPStemmer\n",
    "    nltk.download('rslp')\n",
    "except:\n",
    "    !pip install nltk\n",
    "    import nltk\n",
    "    #nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.rslp import RSLPStemmer\n",
    "    nltk.download('rslp')\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.model_selection import ParameterGrid\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn import svm\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.datasets import make_blobs\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from sklearn.model_selection import KFold\n",
    "except ModuleNotFoundError:\n",
    "    !pip install scikit-learn\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.model_selection import ParameterGrid\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn import svm\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.datasets import make_blobs\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from sklearn.model_selection import KFold\n",
    "try:\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "except ModuleNotFoundError:\n",
    "    !pip install numpy\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except ModuleNotFoundError:\n",
    "    !pip install tensorflow\n",
    "    import tensorflow as tf\n",
    "try:\n",
    "    from keras.models import Sequential\n",
    "    from keras import layers\n",
    "except ModuleNotFoundError:\n",
    "    !pip install keras\n",
    "    from keras.models import Sequential\n",
    "    from keras import layers\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ModuleNotFoundError:\n",
    "    !pip install matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except ModuleNotFoundError:\n",
    "    !pip install seaborn\n",
    "    import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bce916",
   "metadata": {},
   "source": [
    "<h2 style=\"font-size: 19px;color:#6DA0FF;font-family:Georgia, Times, 'Times New Roman', serif;letter-spacing: 3px;font-weight: normal\">2. Inicio del programa</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f5340a",
   "metadata": {},
   "source": [
    "<h3  style='font-family: \"Times New Roman\", Times, serif; font-weight: bold;text-align:center;font-size:14px'>CLASE CONTROLADORVIDEO</h3><p style='font-family: \"Times New Roman\", Times, serif; font-size:14px'>En esta clase se realizará los ajustes para manejar el video recibido y manipularlo.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bb88615",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ControladorVideo:\n",
    "    def __init__(self,enlace): \n",
    "        fb=Firebase('Interfaz/recetastextos/')\n",
    "        self._idvideo = fb.reenumerar()\n",
    "        self.enlacevideo=enlace\n",
    "        self.yt=YouTube(self.enlacevideo)\n",
    "        self.nombrevideo=''\n",
    "        self.titulovideo=self.yt.title\n",
    "        self.autorvideo=self.yt.author\n",
    "        self.fechavideo=self.yt.publish_date\n",
    "        self.duracionvideo=self.yt.length\n",
    "        self.rec=RecursosAdicionales()\n",
    "    \"\"\"|DESCARGAR VIDEO URL: descarga el video de youtube\n",
    "       |return: devuelve una ruta absoluta\"\"\"\n",
    "    def descargarVideoURL(self):\n",
    "        recetasVideos = 'recetasvideos/'\n",
    "        #aqui creo un nuevo id para el nuevo video\n",
    "        self._idvideo= self._idvideo+1\n",
    "        #esta sera el archivo del video y su nuevo nombre\n",
    "        nombre='receta'+str(self._idvideo)\n",
    "        #le pedimos al pytube que solo nos descargue el audio y lo descargamos\n",
    "        t=self.yt.streams.filter(file_extension='mp4').first().download(output_path=recetasVideos,filename=nombre+'.mp4')\n",
    "        #devolvemos el nombre\n",
    "        return nombre\n",
    "    \"\"\"|PARSEO VIDEO: pasa el video de .mp4 a .wav\n",
    "       |nombre: es un string que se colocara el nombre del video\n",
    "       |return: devuelve el nuevo nombre del audio en .wav\"\"\"\n",
    "    def parseoVideo(self,nombre):\n",
    "        recetasVideos = 'recetasvideos/'\n",
    "        #tomamos el video en mp4 \n",
    "        track = mp.VideoFileClip(recetasVideos+nombre+'.mp4')\n",
    "        #cambiamos el video a .wav\n",
    "        nombre_wav=\"{}.wav\".format(nombre)\n",
    "        track.audio.write_audiofile(recetasVideos+nombre_wav)\n",
    "        track.close()\n",
    "        return nombre\n",
    "    \"\"\"|SPEECH TEXT:Transforma el audio a texto\n",
    "       |nombre: es un string que se colocara el nombre del video\n",
    "       |return: devuelve un string con el texto devuelto\"\"\"\n",
    "    def speech_text(self,nombre):\n",
    "        recetasVideos = 'recetasvideos/'\n",
    "        #instanciamos el recognizer\n",
    "        r = sr.Recognizer()\n",
    "        audio = sr.AudioFile(recetasVideos+nombre)\n",
    "        with audio as source:\n",
    "            audio_file = r.record(source)\n",
    "        #transcribimos el audio a texto\n",
    "        result = r.recognize_google(audio_file, language = 'es-ES')\n",
    "        return result\n",
    "    def data_json(self):\n",
    "        return {\"id\":self._idvideo, \"nombre\":self.titulovideo, \"autor\": self.autorvideo, \"fecha\":str(self.fechavideo),\"enlace\":str(self.enlacevideo)}\n",
    "    def indexar_datos(self):\n",
    "        return self.rec.indexar_datos(\"Interfaz/recetastextos/indice.json\",{\"id\":self._idvideo+1, \"nombre\":self.titulovideo, \"autor\": self.autorvideo, \"fecha\":str(self.fechavideo),\"enlace\":str(self.enlacevideo)})\n",
    "    \"\"\"|REPETIDO:Nos dice si el video ya se encuentra en nuestra bd\n",
    "       |fileName: nombre del json\n",
    "       |key: llave en donde queremos encontrar lo que buscamos\n",
    "       |buscar: elemento que estamos buscando\"\"\"\n",
    "    def repetido(self):\n",
    "        return self.rec.buscar_json('Interfaz/recetastextos/indice.json','nombre',self.titulovideo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b297ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3  style='font-family: \"Times New Roman\", Times, serif; font-weight: bold;text-align:center;font-size:14px'>CLASE DEPURADOR</h3><p style='font-family: \"Times New Roman\", Times, serif; font-size:14px'>En esta clase se realizará el proceso de extraccion, transformacion y carga de nuestro programa EATEASER.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d0598a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#si el video es mayor de 3 minutos no funciona\n",
    "#si el video es en ingles no funciona\n",
    "class Depurador:\n",
    "    \n",
    "    def __init__(self): \n",
    "        self.rec=RecursosAdicionales()\n",
    "    \"\"\"|VIDEO: proceso etl donde extraemos al informacion del video \n",
    "       |enlace: es un string que se colocara el enlace del video\"\"\"\n",
    "    def filtroDescarga(self, enlace_txtbox):\n",
    "        if(re.search(\"\\/playlist\\?\", enlace_txtbox)):\n",
    "            self.lista(enlace_txtbox)\n",
    "        else:\n",
    "            self.video(enlace_txtbox)\n",
    "    def video(self,enlace):\n",
    "        try:\n",
    "            #instanciamos el controlador de videos\n",
    "            cv=ControladorVideo(enlace)\n",
    "            fb=Firebase('Interfaz/recetastextos/')\n",
    "            \n",
    "            #paso 1: verificamos si existe en la database\n",
    "            if fb.validar_database(cv.titulovideo)==False:\n",
    "                #paso 2: guardamos en database datos principales\n",
    "                \n",
    "                #paso 3: descargamos el video\n",
    "                cv.nombrevideo=cv.descargarVideoURL()\n",
    "                print(\"id: \"+str(cv._idvideo))\n",
    "                fb.guardar_database(cv.data_json(),cv._idvideo)\n",
    "                #paso 4: pasamos el video a .wav\n",
    "                nombre=cv.parseoVideo(cv.nombrevideo)\n",
    "                #paso 5: evaluamos los silencios \n",
    "                try:                \n",
    "                    num_segm=self.rec.segcionarXsilencios(nombre)\n",
    "                    result=\"\"\n",
    "                    for i in range(num_segm):\n",
    "                        try:\n",
    "                            result=result+str(cv.speech_text(\"../temp_audios/{}_extracto{}.wav\".format(nombre,i+1)))\n",
    "                            result=result+\" \"\n",
    "                        except BaseException:\n",
    "                            logging.exception(\"An exception was thrown!\")\n",
    "                            audio1=AudioSegment.from_wav(\"temp_audios/{}_extracto{}.wav\".format(nombre,i+1))\n",
    "                            duracion=audio1.duration_seconds\n",
    "                            if duracion<=5:\n",
    "                                print(\"El extracto {} es un silencio\".format(i+1))\n",
    "                            elif duracion<=180:\n",
    "                                print(\"El extracto {} es música o ruido\".format(i+1))\n",
    "                            else:\n",
    "                                print(\"Error importante en el extracto {}\".format(i+1))\n",
    "                    #paso 6: borramos los chunks temporales de audio\n",
    "                    self.rec.eliminacion_audio(\"temp_audios\",\"wav\")\n",
    "                    try:\n",
    "                        quitarEmojis = dict.fromkeys(range(0x10000, sys.maxunicode + 1), 'NULL')\n",
    "                        tituloSinEmojis=cv.titulovideo.translate(quitarEmojis)\n",
    "                        autorSinEmojis=cv.autorvideo.translate(quitarEmojis)\n",
    "                        #paso 7: escribimos el texto recibido en un txt->se guarda en local\n",
    "                        resultado=self.rec.escritura(cv.nombrevideo,\"Titulo:\"+tituloSinEmojis+\"\\n\"+\"Autor:\"+autorSinEmojis+\"\\n\"+\"Fecha Publicacion:\"+str(cv.fechavideo)+\"\\n\"+\"Enlace: \"+str(cv.enlacevideo)+\"\\n\"+\"Entradilla:\"+result)\n",
    "                        #paso 8: guardamos el texto en una base de datos\n",
    "                        fb.guardar_firebase(cv.nombrevideo+'.txt')\n",
    "                        #paso 9: eliminamos los mp4\n",
    "                        self.rec.eliminacion_audio(\"recetasvideos\",\"mp4\")\n",
    "                    except BaseException:\n",
    "                        logging.exception(\"An exception was thrown!\")\n",
    "                        print(\"No se ha podido eliminar los caracteres corruptos el video: \"+ cv.nombrevideo + \" - \"+ cv.titulovideo)\n",
    "                        self.rec.eliminacion_audio(\"recetasvideos\",\"mp4\")\n",
    "                        return None   \n",
    "                except BaseException:\n",
    "                    logging.exception(\"An exception was thrown!\")\n",
    "                    print(\"No se ha podido transcribir el video: \"+ cv.nombrevideo + \" - \"+ cv.titulovideo+\" - \"+cv.enlacevideo)\n",
    "                    self.rec.eliminacion_audio(\"recetasvideos\",\"mp4\")\n",
    "                    self.rec.eliminacion_audio(\"temp_audios\",\"wav\")\n",
    "                    return None\n",
    "            else:\n",
    "                print('Este video se encuentra en la base de datos.')\n",
    "                resultado=\"\"\n",
    "            return resultado\n",
    "        except BaseException:\n",
    "            logging.exception(\"An exception was thrown!\")\n",
    "            print(\"No se ha podido descargar el video: \"+ cv.nombrevideo + \" - \"+ cv.titulovideo)\n",
    "            return None\n",
    "    def lista(self, enlace):\n",
    "        playlist_urls = Playlist(enlace)\n",
    "        for url in playlist_urls:\n",
    "            self.video(url)\n",
    "    def transformacion(self):\n",
    "        print()\n",
    "    def carga(self):\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c910592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aqui iran lecturas-escrituras-guardar-eliminar cosas en bases de datos\n",
    "class RecursosAdicionales:\n",
    "    \"\"\"|ESCRITURA: escribe textos txt\n",
    "       |nombre: nombre del \n",
    "       |return: devuelve el audio en texto\"\"\"    \n",
    "    def escritura(self,nombre,texto):\n",
    "        recetasTextos = './Interfaz/recetastextos/'\n",
    "        if not(os.path.exists(recetasTextos)):\n",
    "            os.mkdir(recetasTextos)\n",
    "        f = open(recetasTextos+nombre+'.txt', 'w')\n",
    "        f.write(texto)\n",
    "        f = open(recetasTextos+nombre+'.txt', \"r\")\n",
    "        print(f.read())\n",
    "        f.close()\n",
    "        \n",
    "    def lectura_json(self,fileName):\n",
    "        if self.documento_vacio(fileName):\n",
    "            with open(fileName, \"r\") as file:\n",
    "                    archivo=json.load(file)\n",
    "        else: \n",
    "            archivo=[]\n",
    "            print('El documento se encuentra vacio.')\n",
    "        return archivo\n",
    "    \n",
    "    def escritura_json(self,fileName,data):\n",
    "        with open(fileName, \"w\") as file:\n",
    "                json.dump(data, file)\n",
    "                file.close()\n",
    "    def buscar_json(self,fileName,key,buscar):\n",
    "        encontrado=False\n",
    "        if self.documento_vacio(fileName):\n",
    "            archivo_json=self.lectura_json(fileName)\n",
    "            for item in archivo_json:\n",
    "                if buscar in item[key]:\n",
    "                    print('encontrado')\n",
    "                    encontrado=True\n",
    "                    #no me gusta usar esto pero no tengo idea de como usar un while con json\n",
    "                    break\n",
    "        return encontrado\n",
    "    def documento_vacio(self,fileName):\n",
    "        return os.stat(fileName).st_size != 0\n",
    "    def indexar_datos(self,fileName,adicion):\n",
    "        if not(os.path.exists(fileName)):\n",
    "            os.mkdir(fileName)\n",
    "        data=[]\n",
    "        data=self.lectura_json(fileName)\n",
    "        data.append(adicion)\n",
    "        self.escritura_json(fileName,data)\n",
    "        \n",
    "    def eliminacion_audio(self,path,tipo):\n",
    "        url = './'+path+'/'\n",
    "        py_files = glob.glob(url+'*.'+tipo)\n",
    "        for py_file in py_files:\n",
    "            try:\n",
    "                os.remove(py_file)\n",
    "            except OSError as e:\n",
    "                print(f\"Error:{ e.strerror}\")\n",
    "    \n",
    "    def segcionarXsilencios(self,audio):\n",
    "        audio1=AudioSegment.from_wav(\"./recetasvideos/\"+audio+\".wav\")\n",
    "        var_min=1900\n",
    "        salir=False\n",
    "        while salir==False:\n",
    "            samples = audio1.get_array_of_samples()\n",
    "            segundo=88521\n",
    "            index=[]\n",
    "            for i in range(0,len(samples),int(segundo/5)):\n",
    "                dataSeg = samples[i:int(segundo/5)+i]\n",
    "                media=np.mean(dataSeg)\n",
    "                var=np.var(dataSeg)\n",
    "                if -10<=media<=10 and var<=var_min:\n",
    "                    index.append(i)\n",
    "\n",
    "            borrar=[]\n",
    "            guardado=0\n",
    "            for i in range(len(index)-1):\n",
    "                if index[i+1]<=index[i]+(20*segundo):\n",
    "                    if i==0:\n",
    "                        tiempo=(index[i])/segundo\n",
    "                    else:\n",
    "                        tiempo=(index[i+1]-guardado)/segundo\n",
    "                    if tiempo<=120:\n",
    "                        borrar.append(i)\n",
    "                    else:\n",
    "                        guardado=index[i]\n",
    "                else:\n",
    "                    guardado=index[i]\n",
    "\n",
    "            final=np.delete(index, borrar, axis=0) \n",
    "            extractos=[]\n",
    "            if len(final)==0:\n",
    "                var_min=var_min*10\n",
    "                salir=False\n",
    "            else:\n",
    "                for i in range(len(final)):\n",
    "                    if i==0:\n",
    "                        extractos.append(samples[:final[i]])\n",
    "                    else:\n",
    "                        extractos.append(samples[final[i-1]:final[i]])\n",
    "                extractos.append(samples[final[i]:])\n",
    "                salir=True\n",
    "\n",
    "        for i in range(len(extractos)):\n",
    "            nombre=\"\"\n",
    "            new_sound = audio1._spawn(extractos[i])\n",
    "            nombre=\"temp_audios/{}_extracto{}.wav\".format(audio,i+1)\n",
    "            new_sound.export(nombre,format=\"wav\")\n",
    "        #print(len(extractos))\n",
    "        return len(extractos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "131f35f6-ac5b-4c99-88a3-e47d738b6a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Firebase:\n",
    "    def __init__(self,ubicacion):\n",
    "        self.ubi=ubicacion\n",
    "        \n",
    "        self.config={\"apiKey\": \"AIzaSyDDg9WOlFJxnEJoxomYtsnkJfsI4TgoL_E\",\"authDomain\": \"eateaser-741d4.firebaseapp.com\",\"databaseURL\" : \"https://eateaser-741d4-default-rtdb.firebaseio.com/\",\"projectId\": \"eateaser-741d4\",\"storageBucket\": \"eateaser-741d4.appspot.com\",\"messagingSenderId\": \"706351391410\",\"appId\": \"1:706351391410:web:6abc2cabd6bf83843b5fab\",\"measurementId\": \"G-YZZCBRHNBT\"};\n",
    "        self.firebase=self.conexion_firebase()\n",
    "        self.database=self.firebase.database()\n",
    "    def conexion_firebase(self):\n",
    "        return pyrebase.initialize_app(self.config)\n",
    "    def guardar_firebase(self,nom):\n",
    "        storage=self.firebase.storage()\n",
    "        storage.child(self.ubi+nom).put(self.ubi+nom)\n",
    "    def eliminar_firebase(self,nom):\n",
    "        self.firebase.storage().delete(self.ubi+nom)\n",
    "    def guardar_database(self,data,_id):\n",
    "        self.database.child('Recetas').child(_id).set(data)\n",
    "    def validar_database(self,data):\n",
    "        validar=self.database.get()\n",
    "        encontrado=False\n",
    "        for a in validar.each():\n",
    "            if  data in str(a.val()):\n",
    "                encontrado=True\n",
    "                #no me gusta usar esto pero no tengo idea de como usar un while con json\n",
    "                break\n",
    "        return encontrado\n",
    "    def reenumerar(self):\n",
    "        recetas=self.database.child(\"Recetas\").get()\n",
    "        id=0\n",
    "        for item in recetas.each():\n",
    "            id=item.key()\n",
    "        return int(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e747077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScrap:\n",
    "    def __init__(self): \n",
    "        self.headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    def request(self, url):\n",
    "        request1 = requests.get(url, headers=self.headers)\n",
    "        html = request1.content\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        return soup\n",
    "    def verificar_alimento(self,alimento):\n",
    "        soup = self.request( 'https://www.themealdb.com/api/json/v1/1/search.php?s='+alimento)\n",
    "        print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f517c7a4",
   "metadata": {},
   "source": [
    "<h2 style=\"font-size: 19px;color:#6DA0FF;font-family:Georgia, Times, 'Times New Roman', serif;letter-spacing: 3px;font-weight: normal\">3. Main</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab0f882",
   "metadata": {},
   "source": [
    "#y si tambien vemos si le permitimos al usuario que meta videos?\n",
    "dep=Depurador()\n",
    "if __name__ == '__main__':\n",
    "    #dep.video('https://www.youtube.com/watch?v=6PzQY1E2s2g&list=PLxHmjpcgU5ArC2rY5cpoIcZoVKB_0UHfR&ab_channel=PlatosF%C3%A1cilesconTamara')\n",
    "    #dep.video('https://www.youtube.com/watch?v=PsqR5M8rdjA&list=LL&index=9&t=4s')\n",
    "    #dep.video('https://www.youtube.com/watch?v=xfYcM_jHgPY')\n",
    "    #dep.video('https://www.youtube.com/watch?v=wiCfqc5W-yo')\n",
    "    #error_nuevo#dep.video('https://www.youtube.com/watch?v=3DnPkf9rP_0')\n",
    "    #error_nuevo#dep.video('https://www.youtube.com/watch?v=xVsgKMZFCZY')\n",
    "    #dep.video('https://www.youtube.com/watch?v=rpCe0RPMY94')\n",
    "    #dep.video('https://www.youtube.com/watch?v=rv4gLMa-FYk')\n",
    "    #dep.video('https://www.youtube.com/watch?v=VS8zYxBj4r8')\n",
    "    #dep.video('https://www.youtube.com/watch?v=o99JXrEkZoo')\n",
    "    #dep.video('https://www.youtube.com/watch?v=lKkg5L23b3M')\n",
    "    #dep.video('https://www.youtube.com/watch?v=PsqR5M8rdjA&t=14s')\n",
    "    #dep.video('https://www.youtube.com/watch?v=IvZaAL6qYe0&t=29s')\n",
    "    #dep.video('https://www.youtube.com/watch?v=SIMQBuuyE9M')\n",
    "    #dep.video('https://www.youtube.com/watch?v=_YoZfg7R8Hk')\n",
    "    #dep.video('https://www.youtube.com/watch?v=Zv7KdlOBk7Y')\n",
    "    #dep.video('https://www.youtube.com/watch?v=mFcN4btaZyI&t=2s')\n",
    "    #dep.video('https://www.youtube.com/watch?v=sRmmQBBln9Q')\n",
    "    #dep.video('https://www.youtube.com/watch?v=-QoTJJJfeEE')\n",
    "    #dep.video('https://www.youtube.com/watch?v=JRY5obPKPzo&list=PLxHmjpcgU5ArC2rY5cpoIcZoVKB_0UHfR&index=5&ab_channel=PlatosF%C3%A1cilesconTamara')\n",
    "    #dep.video('https://www.youtube.com/watch?v=stFmx7OCy1k&ab_channel=RecetasdeEsbieta')\n",
    "    #error_videomuylargo#dep.video(\"https://www.youtube.com/watch?v=qqTqePGIjhc\")\n",
    "    #dep.lista(\"https://youtube.com/playlist?list=PLQwfLPYiFlOsS9x6zgeZmFRLqDx3poZvw\")\n",
    "    #dep.lista(\"https://www.youtube.com/playlist?list=PLIsSIvqffHZvM2v1QS5Zi0MUL258EKLPq\")\n",
    "    #dep.video('https://www.youtube.com/watch?v=rv4gLMa-FYk')\n",
    "    \n",
    "    #dep.lista(\"https://youtube.com/playlist?list=PLf2b-1EmxBEcmcj5GPFfFMbvegVKFOIYR\")\n",
    "    #dep.lista(\"https://youtube.com/playlist?list=PL2rWPa7BVMtzadghDZ7cHbkXuJ735RVnZ\")\n",
    "    #dep.lista(\"https://youtube.com/playlist?list=PLiIutYe2uQJrwuRzF0_8tf_a651emeOiO\")\n",
    "    #dep.lista(\"https://youtube.com/playlist?list=PLEOkiu1MfX7FsiTlZfaHZtMfo1EZD96tq\")\n",
    "    #dep.lista(\"https://youtube.com/playlist?list=PL8Vs-hI7gkl0yY6T0qbWSsw_Zv9d2cqnu\")#arroces\n",
    "    #dep.lista(\"https://youtube.com/playlist?list=PL8Id0yl_4Lo-AtOvrizH3OA6yOK0HLRPw\")#mariscos\n",
    "    #dep.lista(\"https://youtube.com/playlist?list=PLxHmjpcgU5Apmx0uz4mfhWZmMFrIm-1a8\")#pasta **\n",
    "    #dep.lista(\"https://youtube.com/playlist?list=PLxHmjpcgU5AqiG1XoX00meJj9rnNIp9qT\")#carnes\n",
    "    #dep.lista(\"https://youtube.com/playlist?list=PLgDn1_a8qclShfo0yvUUPrX673yC3v8LR\")#pescados y algun marisco\n",
    "    #No acabada # dep.lista(\"https://youtube.com/playlist?list=PLge9wrsFXyuYHweFYpDMnHp95WYe6Prfn\")#verduras\n",
    "    #dep.lista(\"https://youtube.com/playlist?list=PL1DDoU1JPaGI0ZVkGbXPhUq2ttCVPBKc5\")#arroces\n",
    "    #dep.lista(\"https://youtube.com/playlist?list=PL75JfQSBdGa9ez55vz1evAkKtI_e8SzCh\")\n",
    "    #dep.lista(\"https://youtube.com/playlist?list=PL75JfQSBdGa9RP3HprHJHyLMmM2hC9uu7\")#marisco\n",
    "    #dep.lista(\"https://youtube.com/playlist?list=PLNvWgJIx6X41jbPxHH0h6I_JJimIKzlVF\")#pasta\n",
    "    #dep.lista(\"https://youtube.com/playlist?list=PLcPdHx9MSg_DAHTy258b0F1vdZE7nRHYX\")#pescado\n",
    "    #dep.lista(\"https://youtube.com/playlist?list=PLWwMSMcUrXKFkBuQfR0uNB7E8TL7dnmfY\")#platosMenores\n",
    "    #dep.lista(\"https://youtube.com/playlist?list=PLdLEn2GksKhaCS9QmsKaHcC4Yr6jCKHf_\")#verduras\n",
    "    #dep.lista(\"https://youtube.com/playlist?list=PLoNzD53SxXZC84LG74DVYvqYCSwMaVIKn\")#platosMenores\n",
    "    #dep.lista(\"https://youtube.com/playlist?list=PLUxqTjTdTvkN2JpDMcTKcMxEGZqmJ14Xu\")#platosMenores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cfa42e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0./Interfaz/recetastextos/Carpeta Arroz/-----------------\n",
      "['receta108.txt', 'receta12.txt', 'receta125.txt', 'receta133.txt', 'receta134.txt', 'receta139.txt', 'receta160.txt', 'receta163.txt', 'receta165.txt', 'receta171.txt', 'receta172.txt', 'receta173.txt', 'receta174.txt', 'receta175.txt', 'receta177.txt', 'receta200.txt', 'receta358.txt', 'receta359.txt', 'receta360.txt', 'receta363.txt', 'receta364.txt', 'receta365.txt', 'receta366.txt', 'receta367.txt', 'receta368.txt', 'receta374.txt', 'receta376.txt', 'receta379.txt', 'receta386.txt', 'receta388.txt', 'receta389.txt', 'receta390.txt', 'receta391.txt', 'receta392.txt', 'receta393.txt', 'receta394.txt', 'receta395.txt', 'receta396.txt', 'receta397.txt', 'receta398.txt', 'receta400.txt', 'receta401.txt', 'receta402.txt', 'receta405.txt', 'receta406.txt', 'receta407.txt', 'receta410.txt', 'receta411.txt', 'receta412.txt', 'receta416.txt', 'receta417.txt', 'receta422.txt', 'receta429.txt', 'receta84.txt', 'receta90.txt']\n",
      "1./Interfaz/recetastextos/Carpeta Bebidas/-----------------\n",
      "['noBorrar.txt', 'receta623.txt', 'receta633.txt', 'receta634.txt', 'receta635.txt', 'receta636.txt', 'receta637.txt', 'receta638.txt', 'receta639.txt', 'receta640.txt', 'receta641.txt', 'receta642.txt', 'receta643.txt', 'receta644.txt', 'receta645.txt', 'receta646.txt', 'receta648.txt', 'receta649.txt', 'receta651.txt', 'receta652.txt', 'receta653.txt', 'receta654.txt', 'receta655.txt', 'receta656.txt', 'receta657.txt', 'receta658.txt', 'receta659.txt', 'receta660.txt', 'receta661.txt', 'receta662.txt', 'receta663.txt', 'receta664.txt', 'receta665.txt', 'receta666.txt', 'receta668.txt', 'receta669.txt', 'receta670.txt', 'receta671.txt', 'receta672.txt', 'receta674.txt', 'receta675.txt', 'receta676.txt', 'receta678.txt', 'receta679.txt', 'receta680.txt', 'receta681.txt', 'receta682.txt', 'receta683.txt', 'receta685.txt', 'receta686.txt', 'receta687.txt', 'receta688.txt', 'receta690.txt', 'receta692.txt', 'receta693.txt', 'receta694.txt', 'receta695.txt', 'receta696.txt', 'receta698.txt', 'receta700.txt', 'receta701.txt', 'receta702.txt', 'receta703.txt']\n",
      "2./Interfaz/recetastextos/Carpeta Carnes/-----------------\n",
      "['receta1.txt', 'receta114.txt', 'receta16.txt', 'receta209.txt', 'receta258.txt', 'receta259.txt', 'receta260.txt', 'receta261.txt', 'receta262.txt', 'receta263.txt', 'receta264.txt', 'receta266.txt', 'receta267.txt', 'receta268.txt', 'receta269.txt', 'receta270.txt', 'receta271.txt', 'receta272.txt', 'receta273.txt', 'receta274.txt', 'receta275.txt', 'receta276.txt', 'receta277.txt', 'receta278.txt', 'receta280.txt', 'receta281.txt', 'receta282.txt', 'receta283.txt', 'receta284.txt', 'receta285.txt', 'receta286.txt', 'receta287.txt', 'receta29.txt', 'receta3.txt', 'receta31.txt', 'receta32.txt', 'receta39.txt', 'receta42.txt', 'receta43.txt', 'receta48.txt', 'receta51.txt', 'receta58.txt', 'receta6.txt', 'receta63.txt', 'receta65.txt', 'receta71.txt', 'receta73.txt', 'receta77.txt', 'receta781.txt', 'receta782.txt', 'receta783.txt', 'receta784.txt', 'receta785.txt', 'receta786.txt', 'receta787.txt', 'receta788.txt', 'receta789.txt', 'receta790.txt', 'receta791.txt', 'receta792.txt', 'receta793.txt', 'receta794.txt', 'receta795.txt', 'receta796.txt', 'receta797.txt', 'receta798.txt', 'receta799.txt', 'receta8.txt', 'receta800.txt', 'receta801.txt', 'receta802.txt', 'receta803.txt', 'receta804.txt', 'receta805.txt', 'receta806.txt', 'receta807.txt', 'receta808.txt', 'receta809.txt', 'receta810.txt', 'receta811.txt', 'receta812.txt', 'receta83.txt', 'receta871.txt', 'receta872.txt', 'receta873.txt', 'receta874.txt', 'receta875.txt', 'receta876.txt', 'receta877.txt', 'receta878.txt', 'receta879.txt', 'receta880.txt', 'receta881.txt', 'receta882.txt', 'receta883.txt', 'receta884.txt', 'receta885.txt', 'receta886.txt', 'receta887.txt', 'receta888.txt', 'receta889.txt', 'receta890.txt', 'receta891.txt', 'receta892.txt', 'receta893.txt', 'receta894.txt', 'receta895.txt', 'receta896.txt', 'receta897.txt', 'receta898.txt', 'receta899.txt', 'receta900.txt', 'receta901.txt', 'receta902.txt', 'receta903.txt', 'receta904.txt', 'receta905.txt', 'receta906.txt', 'receta907.txt', 'receta908.txt', 'receta909.txt', 'receta910.txt', 'receta911.txt', 'receta912.txt', 'receta913.txt', 'receta914.txt', 'receta915.txt', 'receta916.txt', 'receta917.txt', 'receta918.txt', 'receta919.txt', 'receta920.txt', 'receta93.txt']\n",
      "3./Interfaz/recetastextos/Carpeta Marisco/-----------------\n",
      "['receta180.txt', 'receta184.txt', 'receta185.txt', 'receta195.txt', 'receta202.txt', 'receta203.txt', 'receta204.txt', 'receta205.txt', 'receta212.txt', 'receta213.txt', 'receta214.txt', 'receta216.txt', 'receta218.txt', 'receta219.txt', 'receta221.txt', 'receta224.txt', 'receta226.txt', 'receta26.txt', 'receta298.txt', 'receta306.txt', 'receta310.txt', 'receta319.txt', 'receta322.txt', 'receta330.txt', 'receta333.txt', 'receta334.txt', 'receta35.txt', 'receta41.txt', 'receta481.txt', 'receta486.txt', 'receta487.txt', 'receta491.txt', 'receta5.txt', 'receta501.txt', 'receta502.txt', 'receta505.txt', 'receta510.txt', 'receta515.txt', 'receta516.txt', 'receta517.txt', 'receta518.txt', 'receta520.txt', 'receta521.txt', 'receta522.txt', 'receta524.txt', 'receta525.txt', 'receta528.txt', 'receta532.txt', 'receta540.txt', 'receta541.txt', 'receta542.txt', 'receta544.txt', 'receta552.txt', 'receta602.txt', 'receta632.txt']\n",
      "4./Interfaz/recetastextos/Carpeta Pasta/-----------------\n",
      "['receta118.txt', 'receta120.txt', 'receta15.txt', 'receta228.txt', 'receta229.txt', 'receta230.txt', 'receta231.txt', 'receta232.txt', 'receta234.txt', 'receta235.txt', 'receta236.txt', 'receta237.txt', 'receta238.txt', 'receta239.txt', 'receta24.txt', 'receta240.txt', 'receta241.txt', 'receta242.txt', 'receta243.txt', 'receta244.txt', 'receta246.txt', 'receta247.txt', 'receta248.txt', 'receta249.txt', 'receta250.txt', 'receta252.txt', 'receta253.txt', 'receta254.txt', 'receta256.txt', 'receta257.txt', 'receta555.txt', 'receta556.txt', 'receta557.txt', 'receta558.txt', 'receta559.txt', 'receta56.txt', 'receta561.txt', 'receta563.txt', 'receta564.txt', 'receta566.txt', 'receta567.txt', 'receta568.txt', 'receta569.txt', 'receta570.txt', 'receta573.txt', 'receta574.txt', 'receta575.txt', 'receta576.txt', 'receta577.txt', 'receta578.txt', 'receta579.txt', 'receta580.txt', 'receta581.txt', 'receta582.txt', 'receta583.txt', 'receta584.txt', 'receta586.txt', 'receta587.txt', 'receta588.txt', 'receta589.txt', 'receta590.txt', 'receta591.txt', 'receta592.txt', 'receta813.txt', 'receta814.txt', 'receta815.txt', 'receta816.txt', 'receta817.txt', 'receta818.txt', 'receta819.txt', 'receta820.txt', 'receta941.txt', 'receta942.txt', 'receta943.txt', 'receta944.txt', 'receta945.txt', 'receta946.txt', 'receta947.txt', 'receta948.txt', 'receta949.txt', 'receta95.txt', 'receta950.txt', 'receta951.txt', 'receta952.txt', 'receta953.txt', 'receta954.txt', 'receta955.txt', 'receta956.txt', 'receta957.txt', 'receta958.txt', 'receta959.txt', 'receta960.txt', 'receta961.txt', 'receta962.txt', 'receta963.txt', 'receta964.txt', 'receta965.txt', 'receta966.txt', 'receta967.txt', 'receta968.txt', 'receta969.txt', 'receta970.txt', 'receta971.txt', 'receta972.txt', 'receta973.txt', 'receta974.txt', 'receta975.txt', 'receta976.txt', 'receta977.txt', 'receta978.txt', 'receta979.txt', 'receta980.txt', 'receta981.txt', 'receta982.txt', 'receta983.txt', 'receta984.txt', 'receta985.txt', 'receta986.txt', 'receta987.txt', 'receta988.txt', 'receta989.txt', 'receta990.txt']\n",
      "5./Interfaz/recetastextos/Carpeta Pescados/-----------------\n",
      "['receta1000.txt', 'receta1001.txt', 'receta1002.txt', 'receta1003.txt', 'receta1004.txt', 'receta1005.txt', 'receta1006.txt', 'receta1007.txt', 'receta1008.txt', 'receta1009.txt', 'receta1010.txt', 'receta1011.txt', 'receta1012.txt', 'receta1013.txt', 'receta1014.txt', 'receta1015.txt', 'receta1016.txt', 'receta1017.txt', 'receta1018.txt', 'receta1019.txt', 'receta1020.txt', 'receta1021.txt', 'receta1022.txt', 'receta1023.txt', 'receta1024.txt', 'receta1025.txt', 'receta1026.txt', 'receta1027.txt', 'receta1028.txt', 'receta1029.txt', 'receta1030.txt', 'receta1031.txt', 'receta1032.txt', 'receta1033.txt', 'receta1034.txt', 'receta1035.txt', 'receta1036.txt', 'receta1037.txt', 'receta1038.txt', 'receta1039.txt', 'receta1040.txt', 'receta11.txt', 'receta123.txt', 'receta181.txt', 'receta190.txt', 'receta194.txt', 'receta25.txt', 'receta297.txt', 'receta299.txt', 'receta301.txt', 'receta304.txt', 'receta323.txt', 'receta325.txt', 'receta332.txt', 'receta335.txt', 'receta336.txt', 'receta337.txt', 'receta338.txt', 'receta340.txt', 'receta480.txt', 'receta482.txt', 'receta484.txt', 'receta485.txt', 'receta488.txt', 'receta489.txt', 'receta490.txt', 'receta492.txt', 'receta493.txt', 'receta494.txt', 'receta495.txt', 'receta496.txt', 'receta497.txt', 'receta499.txt', 'receta500.txt', 'receta503.txt', 'receta504.txt', 'receta507.txt', 'receta508.txt', 'receta509.txt', 'receta513.txt', 'receta514.txt', 'receta519.txt', 'receta527.txt', 'receta530.txt', 'receta531.txt', 'receta537.txt', 'receta539.txt', 'receta596.txt', 'receta598.txt', 'receta601.txt', 'receta609.txt', 'receta611.txt', 'receta614.txt', 'receta617.txt', 'receta619.txt', 'receta620.txt', 'receta621.txt', 'receta622.txt', 'receta624.txt', 'receta625.txt', 'receta627.txt', 'receta629.txt', 'receta630.txt', 'receta7.txt', 'receta74.txt', 'receta81.txt', 'receta88.txt', 'receta991.txt', 'receta992.txt', 'receta993.txt', 'receta994.txt', 'receta995.txt', 'receta996.txt', 'receta997.txt', 'receta998.txt', 'receta999.txt']\n",
      "6./Interfaz/recetastextos/Carpeta Platos Menores/-----------------\n",
      "['receta112.txt', 'receta14.txt', 'receta18.txt', 'receta2.txt', 'receta227.txt', 'receta27.txt', 'receta30.txt', 'receta38.txt', 'receta4.txt', 'receta47.txt', 'receta50.txt', 'receta53.txt', 'receta57.txt', 'receta60.txt', 'receta61.txt', 'receta69.txt', 'receta704.txt', 'receta705.txt', 'receta706.txt', 'receta711.txt', 'receta714.txt', 'receta716.txt', 'receta718.txt', 'receta72.txt', 'receta720.txt', 'receta721.txt', 'receta736.txt', 'receta737.txt', 'receta738.txt', 'receta739.txt', 'receta740.txt', 'receta741.txt', 'receta742.txt', 'receta743.txt', 'receta745.txt', 'receta746.txt', 'receta747.txt', 'receta748.txt', 'receta749.txt', 'receta75.txt', 'receta750.txt', 'receta751.txt', 'receta752.txt', 'receta754.txt', 'receta755.txt', 'receta757.txt', 'receta759.txt', 'receta760.txt', 'receta762.txt', 'receta763.txt', 'receta764.txt', 'receta765.txt', 'receta766.txt', 'receta767.txt', 'receta768.txt', 'receta769.txt', 'receta770.txt', 'receta771.txt', 'receta772.txt', 'receta773.txt', 'receta774.txt', 'receta775.txt', 'receta776.txt', 'receta777.txt', 'receta778.txt', 'receta779.txt', 'receta78.txt', 'receta780.txt', 'receta821.txt', 'receta822.txt', 'receta823.txt', 'receta824.txt', 'receta825.txt', 'receta826.txt', 'receta827.txt', 'receta828.txt', 'receta829.txt', 'receta830.txt', 'receta831.txt', 'receta832.txt', 'receta833.txt', 'receta834.txt', 'receta835.txt', 'receta836.txt', 'receta837.txt', 'receta838.txt', 'receta839.txt', 'receta840.txt', 'receta841.txt', 'receta842.txt', 'receta843.txt', 'receta844.txt', 'receta845.txt', 'receta846.txt', 'receta847.txt', 'receta848.txt', 'receta849.txt', 'receta850.txt', 'receta851.txt', 'receta852.txt', 'receta853.txt', 'receta854.txt', 'receta855.txt', 'receta856.txt', 'receta857.txt', 'receta858.txt', 'receta859.txt', 'receta860.txt', 'receta861.txt', 'receta862.txt', 'receta863.txt', 'receta864.txt', 'receta865.txt', 'receta866.txt', 'receta867.txt', 'receta868.txt', 'receta869.txt', 'receta87.txt', 'receta870.txt', 'receta9.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7./Interfaz/recetastextos/Carpeta Verduras/-----------------\n",
      "['receta1041.txt', 'receta1042.txt', 'receta1043.txt', 'receta1044.txt', 'receta1045.txt', 'receta1046.txt', 'receta1047.txt', 'receta1048.txt', 'receta1049.txt', 'receta1050.txt', 'receta1051.txt', 'receta1052.txt', 'receta1053.txt', 'receta1054.txt', 'receta1055.txt', 'receta1056.txt', 'receta1057.txt', 'receta1058.txt', 'receta1059.txt', 'receta1060.txt', 'receta1061.txt', 'receta1062.txt', 'receta1063.txt', 'receta1064.txt', 'receta1065.txt', 'receta1066.txt', 'receta1067.txt', 'receta1068.txt', 'receta1069.txt', 'receta1070.txt', 'receta1071.txt', 'receta1072.txt', 'receta1073.txt', 'receta1074.txt', 'receta1075.txt', 'receta1076.txt', 'receta1077.txt', 'receta1078.txt', 'receta1079.txt', 'receta1080.txt', 'receta1081.txt', 'receta1082.txt', 'receta1083.txt', 'receta1084.txt', 'receta1085.txt', 'receta1086.txt', 'receta1087.txt', 'receta1088.txt', 'receta1089.txt', 'receta1090.txt', 'receta1091.txt', 'receta1092.txt', 'receta1093.txt', 'receta1094.txt', 'receta1095.txt', 'receta1096.txt', 'receta1097.txt', 'receta1098.txt', 'receta1099.txt', 'receta1100.txt', 'receta1101.txt', 'receta1102.txt', 'receta1103.txt', 'receta1104.txt', 'receta1105.txt', 'receta1106.txt', 'receta1107.txt', 'receta1108.txt', 'receta1109.txt', 'receta1110.txt', 'receta1111.txt', 'receta1112.txt', 'receta1113.txt', 'receta1114.txt', 'receta1115.txt', 'receta1116.txt', 'receta1117.txt', 'receta1118.txt', 'receta1119.txt', 'receta1120.txt', 'receta1121.txt', 'receta1122.txt', 'receta1123.txt', 'receta1124.txt', 'receta1125.txt', 'receta1126.txt', 'receta1127.txt', 'receta1128.txt', 'receta1129.txt', 'receta1130.txt', 'receta1131.txt', 'receta1132.txt', 'receta1133.txt', 'receta1134.txt', 'receta1135.txt', 'receta1136.txt', 'receta119.txt', 'receta13.txt', 'receta17.txt', 'receta296.txt', 'receta339.txt', 'receta342.txt', 'receta343.txt', 'receta346.txt', 'receta347.txt', 'receta348.txt', 'receta349.txt', 'receta350.txt', 'receta352.txt', 'receta353.txt', 'receta354.txt', 'receta355.txt', 'receta356.txt', 'receta357.txt', 'receta36.txt', 'receta431.txt', 'receta433.txt', 'receta434.txt', 'receta435.txt', 'receta436.txt', 'receta438.txt', 'receta439.txt', 'receta440.txt', 'receta442.txt', 'receta446.txt', 'receta447.txt', 'receta448.txt', 'receta451.txt', 'receta454.txt', 'receta455.txt', 'receta456.txt', 'receta457.txt', 'receta459.txt', 'receta460.txt', 'receta462.txt', 'receta608.txt', 'receta64.txt', 'receta723.txt', 'receta724.txt', 'receta725.txt', 'receta726.txt', 'receta729.txt', 'receta730.txt', 'receta731.txt', 'receta732.txt', 'receta733.txt', 'receta734.txt', 'receta92.txt', 'receta94.txt']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nprint(len(listaTextosCarpeta))\\n#for l in listaTextosCarpeta:\\n#    print('----------------------------------------------------------------------------')\\n#    print(l)\\n#    print('----------------------------------------------------------------------------')\\n#    print('----------------------------------------------------------------------------')\\n\\n\\nstemsArroz=p.tratamientoTextos(listaTextosCarpeta[0])\\nprint(stemsArroz)\\nstemsBebidas=p.tratamientoTextos(listaTextosCarpeta[1])\\nprint(stemsBebidas)\\nstemsCarne=p.tratamientoTextos(listaTextosCarpeta[2])\\nprint(stemsCarne)\\nstemsMarisco=p.tratamientoTextos(listaTextosCarpeta[3])\\nprint(stemsMarisco)\\nstemsPasta=p.tratamientoTextos(listaTextosCarpeta[4])\\nprint(stemsPasta)\\nstemsPescados=p.tratamientoTextos(listaTextosCarpeta[5])\\nprint(stemsPescados)\\nstemsPlatosMenores=p.tratamientoTextos(listaTextosCarpeta[6])\\nprint(stemsPlatosMenores)\\nstemsVerduras=p.tratamientoTextos(listaTextosCarpeta[7])\\nprint(stemsVerduras)\\n\\nprint('----------------------------')\\n\\nlistaTextosTesting=p.lecturaTesting()\\nprint(listaTextosTesting)\\nstemsTesting=p.tratamientoTextos(listaTextosTesting)\\nprint(stemsTesting)\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ProcesarDocumentos:     \n",
    "    def lectura(self):\n",
    "        procDoc=ProcesarDocumentos()\n",
    "        rutaCarpetasPorCategoria = \"./Interfaz/recetastextos/\"\n",
    "        listaCarpetasFinal = []\n",
    "        #estos string nos servirán para guardar todos los textos de los txt por cada una de las carpetas\n",
    "        carpetaArroz = carpetaBebidas = carpetaCarnes = carpetaMarisco = carpetaPasta = carpetaPescados = carpetaPlatosMenores = carpetaVerduras = ''\n",
    "        #sacamos una lista de todas las carpetas\n",
    "        listaCarpetas = os.listdir(rutaCarpetasPorCategoria)\n",
    "        #print(listaCarpetas)\n",
    "        #print(len(listaCarpetas))\n",
    "        #recorremos todas las carpetas\n",
    "        i=0\n",
    "        for lc in listaCarpetas:\n",
    "            #cogemos el nombre de la carpeta y se lo concatenamos a la ruta anterior\n",
    "            rutaPorCarpeta = rutaCarpetasPorCategoria + lc + '/'\n",
    "            print(str(i)+rutaPorCarpeta+'-----------------')\n",
    "            if(i==0):\n",
    "                carpetaArroz = procDoc.resultadoStringCarpeta(rutaPorCarpeta)\n",
    "                #print(carpetaArroz)\n",
    "                listaCarpetasFinal.append(carpetaArroz)\n",
    "            if(i==1):\n",
    "                carpetaBebidas = procDoc.resultadoStringCarpeta(rutaPorCarpeta)\n",
    "                listaCarpetasFinal.append(carpetaBebidas)\n",
    "            if(i==2):\n",
    "                carpetaCarnes = procDoc.resultadoStringCarpeta(rutaPorCarpeta)\n",
    "                listaCarpetasFinal.append(carpetaCarnes)\n",
    "            if(i==3):\n",
    "                carpetaMarisco = procDoc.resultadoStringCarpeta(rutaPorCarpeta)\n",
    "                listaCarpetasFinal.append(carpetaMarisco)\n",
    "            if(i==4):\n",
    "                carpetaPasta = procDoc.resultadoStringCarpeta(rutaPorCarpeta)\n",
    "                listaCarpetasFinal.append(carpetaPasta)\n",
    "            if(i==5):\n",
    "                carpetaPescados = procDoc.resultadoStringCarpeta(rutaPorCarpeta)\n",
    "                listaCarpetasFinal.append(carpetaPescados)\n",
    "            if(i==6):\n",
    "                carpetaPlatosMenores = procDoc.resultadoStringCarpeta(rutaPorCarpeta)\n",
    "                listaCarpetasFinal.append(carpetaPlatosMenores)\n",
    "            if(i==7):\n",
    "                carpetaVerduras = procDoc.resultadoStringCarpeta(rutaPorCarpeta)\n",
    "                listaCarpetasFinal.append(carpetaVerduras)\n",
    "            i=i+1\n",
    "        return listaCarpetasFinal\n",
    "    def lecturaTesting(self):\n",
    "        procDoc=ProcesarDocumentos()\n",
    "        rutaCarpetaTesting = \"./Interfaz/Carpeta Testing/\"\n",
    "        carpetaTesting = procDoc.resultadoStringCarpeta(rutaCarpetaTesting)\n",
    "        return carpetaTesting\n",
    "    def resultadoStringCarpeta(self, rutaPorCarpeta):\n",
    "        strCarpeta=[]\n",
    "        #vemos el contenido de la carpeta en la que estamos iterando\n",
    "        listaTxt = os.listdir(rutaPorCarpeta)\n",
    "        print(listaTxt)\n",
    "        #recorremos todos los archivos de la carpeta\n",
    "        for lt in listaTxt:\n",
    "            #concatenamos la ruta de la carpeta con el nombre de los archivos que contiene esta\n",
    "            rutaTxt = rutaPorCarpeta + lt\n",
    "            #al ir iterando pasaremos por todos los archivos modificando la variable de la ruta para poder hacer un open con ella\n",
    "            #file = open(filename, encoding=\"utf8\")\n",
    "            try:\n",
    "                with open(rutaTxt, 'r') as f: \n",
    "                    #al hacer el open leemos lo que hay dentro del archivo con f.read(), y esto lo guardamos dentro de un string inicializado al inicio del todo\n",
    "                    strCarpeta.append(f.read())\n",
    "            except:\n",
    "                with open(rutaTxt, 'r',encoding=\"utf8\") as f: \n",
    "                    #al hacer el open leemos lo que hay dentro del archivo con f.read(), y esto lo guardamos dentro de un string inicializado al inicio del todo\n",
    "                    strCarpeta.append(f.read())\n",
    "                \n",
    "        return strCarpeta\n",
    "    def leer_stopwords(self, path):\n",
    "        with open(path) as f:\n",
    "            # Lee las stopwords del archivo y las guarda en una lista\n",
    "            mis_stopwords = [line.strip() for line in f]\n",
    "        return mis_stopwords\n",
    "    def tratamientoTextos(self, info):\n",
    "        #Eliminamos posibles horas del titulo\n",
    "        textoSinSimbolos = re.sub(\"\\d+:\\d+:\\d+\", \"\" , info)\n",
    "        #Eliminamos posibles fechas\n",
    "        textoSinSimbolos = re.sub(\"\\d+-\\d+-\\d+\", \"\" , textoSinSimbolos)\n",
    "        #Eliminamos todos los fin de enlace\n",
    "        textoSinSimbolos = re.sub(\"v=.*\", \"\" , textoSinSimbolos)\n",
    "        #Eliminamos todos los simbolos del texto (,.;:?¿!!) etc\n",
    "        textoSinSimbolos = re.sub(\"[^0-9A-Za-z_]\", \" \" , textoSinSimbolos)\n",
    "        #Sacamos todos los tokens del texto y los metemos en una lista\n",
    "        textoTokenizado = nltk.tokenize.word_tokenize(textoSinSimbolos)\n",
    "        #una lista no tiene lower asique pasamos el lower con map a toda la lista\n",
    "        textoMinusculas = (map(lambda x: x.lower(), textoTokenizado))\n",
    "        #Le pasa un stopword de palabras en español a la lista de palabras que le llega\n",
    "        #stop_words_sp = set(stopwords.words('spanish'))\n",
    "        stop_words_sp = self.leer_stopwords(\"./Interfaz/rapidminer/stop_words_spanish.txt\")\n",
    "        pasarStopWords = [i for i in textoMinusculas if i not in stop_words_sp]\n",
    "        #Aplicamos la normalizacion mediante stemming\n",
    "        #SnowStem = nltk.SnowballStemmer(language = 'spanish')\n",
    "        # Crear un objeto SnowballStemmer para el idioma español\n",
    "        stemmer = RSLPStemmer()\n",
    "        listaStems = [stemmer.stem(word) for word in pasarStopWords]\n",
    "        return listaStems\n",
    "\n",
    "\n",
    "p=ProcesarDocumentos()\n",
    "listaTextosCarpeta=p.lectura()\n",
    "\"\"\"\n",
    "print(len(listaTextosCarpeta))\n",
    "#for l in listaTextosCarpeta:\n",
    "#    print('----------------------------------------------------------------------------')\n",
    "#    print(l)\n",
    "#    print('----------------------------------------------------------------------------')\n",
    "#    print('----------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "stemsArroz=p.tratamientoTextos(listaTextosCarpeta[0])\n",
    "print(stemsArroz)\n",
    "stemsBebidas=p.tratamientoTextos(listaTextosCarpeta[1])\n",
    "print(stemsBebidas)\n",
    "stemsCarne=p.tratamientoTextos(listaTextosCarpeta[2])\n",
    "print(stemsCarne)\n",
    "stemsMarisco=p.tratamientoTextos(listaTextosCarpeta[3])\n",
    "print(stemsMarisco)\n",
    "stemsPasta=p.tratamientoTextos(listaTextosCarpeta[4])\n",
    "print(stemsPasta)\n",
    "stemsPescados=p.tratamientoTextos(listaTextosCarpeta[5])\n",
    "print(stemsPescados)\n",
    "stemsPlatosMenores=p.tratamientoTextos(listaTextosCarpeta[6])\n",
    "print(stemsPlatosMenores)\n",
    "stemsVerduras=p.tratamientoTextos(listaTextosCarpeta[7])\n",
    "print(stemsVerduras)\n",
    "\n",
    "print('----------------------------')\n",
    "\n",
    "listaTextosTesting=p.lecturaTesting()\n",
    "print(listaTextosTesting)\n",
    "stemsTesting=p.tratamientoTextos(listaTextosTesting)\n",
    "print(stemsTesting)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dec6bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(listaTextosCarpeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bfe35eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responde con 1 cuando quieras incluir la categoría y 0 cuando no\n",
      "Quieres meter arroces?1\n",
      "Quieres meter bebidas?1\n",
      "Quieres meter carnes?1\n",
      "Quieres meter marisco?1\n",
      "Quieres meter pasta?1\n",
      "Quieres meter pescados?1\n",
      "Quieres meter platos menores?1\n",
      "Quieres meter verduras?1\n",
      "['1', '1', '1', '1', '1', '1', '1', '1']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>receta</th>\n",
       "      <th>clasif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ensal, arroz, mediterr, nea, veran, cociner, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[haz, arroz, band, 10, recet, pas, pas, mir, m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[arroz, mel, sep, gamb, alcachof, recet, cocin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[arroz, manit, cerd, set, recet, cocin, famil,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[paell, bacala, costr, aliol, recet, cocin, fa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>[calabac, n, m, s, delici, com, recet, calabac...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>[fr, cali, tu, decid, com, delic, repoll, rece...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>[sop, past, alem, past, elab, 1, minut, d, as,...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>[recet, crem, calabaz, companion, jord, cruz, ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>[patat, mediterrane, andaluc, videorecet, entr...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>813 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                receta clasif\n",
       "0    [ensal, arroz, mediterr, nea, veran, cociner, ...      0\n",
       "1    [haz, arroz, band, 10, recet, pas, pas, mir, m...      0\n",
       "2    [arroz, mel, sep, gamb, alcachof, recet, cocin...      0\n",
       "3    [arroz, manit, cerd, set, recet, cocin, famil,...      0\n",
       "4    [paell, bacala, costr, aliol, recet, cocin, fa...      0\n",
       "..                                                 ...    ...\n",
       "808  [calabac, n, m, s, delici, com, recet, calabac...      7\n",
       "809  [fr, cali, tu, decid, com, delic, repoll, rece...      7\n",
       "810  [sop, past, alem, past, elab, 1, minut, d, as,...      7\n",
       "811  [recet, crem, calabaz, companion, jord, cruz, ...      7\n",
       "812  [patat, mediterrane, andaluc, videorecet, entr...      7\n",
       "\n",
       "[813 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df=pd.DataFrame()\n",
    "df['receta']=None\n",
    "df['clasif']=None\n",
    "print(\"Responde con 1 cuando quieras incluir la categoría y 0 cuando no\")\n",
    "\n",
    "sel=[]\n",
    "\n",
    "arroz=input(\"Quieres meter arroces?\")\n",
    "sel.append(arroz)\n",
    "bebida=input(\"Quieres meter bebidas?\")\n",
    "sel.append(bebida)\n",
    "carne=input(\"Quieres meter carnes?\")\n",
    "sel.append(carne)\n",
    "marisco=input(\"Quieres meter marisco?\")\n",
    "sel.append(marisco)\n",
    "pasta=input(\"Quieres meter pasta?\")\n",
    "sel.append(pasta)\n",
    "pescado=input(\"Quieres meter pescados?\")\n",
    "sel.append(pescado)\n",
    "plt_men=input(\"Quieres meter platos menores?\")\n",
    "sel.append(plt_men)\n",
    "verdura=input(\"Quieres meter verduras?\")\n",
    "sel.append(verdura)\n",
    "\n",
    "print(sel)\n",
    "\n",
    "\n",
    "for index,content in enumerate(listaTextosCarpeta):\n",
    "    if sel[index]=='1':\n",
    "        for i in range(len(content)):\n",
    "            text=p.tratamientoTextos(listaTextosCarpeta[index][i])\n",
    "            df=df.append({'receta':text,'clasif':index},ignore_index=True)\n",
    "\n",
    "df\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03d5e077",
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario={0:\"Arroz\",1:\"Bebida\" , \"hola\":\"Carne\" , 3:\"Marisco\",4:\"Pasta\",5:\"Pescado\",6:\"Platos menores\",7:\"Verdura\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19787f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=[3,4,12,6,7,8]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "595e2c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "6\n",
      "7\n",
      "8\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "for i in range(13):\n",
    "    if i in pred:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ae7e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesarDocs=ProcesarDocumentos()\n",
    "listaTextosCarpeta=procesarDocs.lectura()\n",
    "\n",
    "for index,content in enumerate(listaTextosCarpeta):\n",
    "    for i in range(len(content)):\n",
    "        text=p.tratamientoTextos(listaTextosCarpeta[index][i])\n",
    "        df=df.append({'receta':text,'clasif':index},ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c330149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelos:\n",
    "    def __init__(self):\n",
    "        self.preprocesamiento()\n",
    "     \n",
    "    def preprocesamiento(self):\n",
    "        \n",
    "        #Se crea la función que vectoriza los array de las recetas (calcula la frecuencia de las palabras) lo que\n",
    "        #convierte una lista de palabras en un array de frecuencias\n",
    "        self.vectorizer = CountVectorizer(analyzer = \"word\",  tokenizer = None, preprocessor = None,  stop_words = None,  max_features = 10000) \n",
    "\n",
    "        #Se separa el set de datos en datos de entrenamiento y de testeo. En este caso se divide en 80%-20%\n",
    "        #Creandose 4 variables -> \n",
    "        #X_train: Conjunto de recetas de entrenamiento (X_cv: en el testeo) \n",
    "        #Y_train: clasificación de las recetas en los datos de entrenamiento (Y_cv: en el testeo)    \n",
    "        self.X_train, self.X_cv, self.Y_train, self.Y_cv = train_test_split(df[\"receta\"], df[\"clasif\"], test_size = 0.2, random_state=42)\n",
    "        self.Y_train=list(self.Y_train)\n",
    "\n",
    "        #Ahora vecrtorizamos X_train y X_cv para poder meterlo en el modelo de clasificación\n",
    "        #Set de entrenamiento\n",
    "        arrayTemp=[]\n",
    "        for i,j in enumerate(self.X_train):           #El fit_transform funciona con string de frases enteras y automáticamente tokeniza las palabras por lo que hayq ue volver a juntar las palabras en una frase\n",
    "            arrayTemp.append(\" \".join(j))\n",
    "        self.X_train = self.vectorizer.fit_transform(arrayTemp)\n",
    "        self.X_train = self.X_train.toarray()\n",
    "\n",
    "        #Set de testeo\n",
    "        arrayTemp=[]\n",
    "        for i,j in enumerate(self.X_cv):\n",
    "            arrayTemp.append(\" \".join(j))\n",
    "        self.X_cv = self.vectorizer.transform(arrayTemp)\n",
    "        self.X_cv = self.X_cv.toarray()\n",
    "        self.Y_train=list(self.Y_train)\n",
    "        self.Y_cv=list(self.Y_cv)\n",
    "        \n",
    "    def Entrenar_RF(self):\n",
    "        # Grid de hiperparámetros evaluados\n",
    "        # ==============================================================================\n",
    "        \n",
    "        print(self.X_train.shape)\n",
    "        param_grid = ParameterGrid(\n",
    "                        {'n_estimators': [1000],\n",
    "                         'max_features': [5, 7, 9],\n",
    "                         'max_depth'   : [None, 3, 10, 20],\n",
    "                         'criterion'   : ['gini', 'entropy']\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        # Loop para ajustar un modelo con cada combinación de hiperparámetros\n",
    "        # ==============================================================================\n",
    "        resultados = {'params': [], 'oob_accuracy': []}\n",
    "\n",
    "        for params in param_grid:\n",
    "\n",
    "            modelo = RandomForestClassifier(\n",
    "                        oob_score    = True,\n",
    "                        n_jobs       = -1,\n",
    "                        random_state = 123,\n",
    "                        ** params\n",
    "                     )\n",
    "\n",
    "            modelo.fit(self.X_train, self.Y_train)\n",
    "\n",
    "            resultados['params'].append(params)\n",
    "            resultados['oob_accuracy'].append(modelo.oob_score_)\n",
    "            print(f\"Modelo: {params} \\u2713\")\n",
    "\n",
    "        # Resultados\n",
    "        # ==============================================================================\n",
    "        resultados = pd.DataFrame(resultados)\n",
    "        resultados = pd.concat([resultados, resultados['params'].apply(pd.Series)], axis=1)\n",
    "        resultados = resultados.sort_values('oob_accuracy', ascending=False)\n",
    "        resultados = resultados.drop(columns = 'params')\n",
    "        print(resultados.head(4))\n",
    "        \n",
    "        '''\n",
    "        self.forest = RandomForestClassifier() \n",
    "        self.forest = self.forest.fit(self.X_train, self.Y_train)\n",
    "\n",
    "        predictions = self.forest.predict(self.X_cv) \n",
    "        self.Y_cv=list(self.Y_cv)\n",
    "        print(\"Accuracy: \", accuracy_score(self.Y_cv, predictions))\n",
    "        '''\n",
    "        \n",
    "    def Entrenar_RF_CV(self):\n",
    "        # Grid de hiperparámetros evaluados\n",
    "        # ==============================================================================\n",
    "        param_grid = {'n_estimators': [150],\n",
    "                      'max_features': [5, 7, 9],\n",
    "                      'max_depth'   : [None, 3, 10, 20],\n",
    "                      'criterion'   : ['gini', 'entropy']\n",
    "                     }\n",
    "\n",
    "        # Búsqueda por grid search con validación cruzada\n",
    "        # ==============================================================================\n",
    "        grid = GridSearchCV(\n",
    "                estimator  = RandomForestClassifier(random_state = 123),\n",
    "                param_grid = param_grid,\n",
    "                scoring    = 'accuracy',\n",
    "                n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "                cv         = RepeatedKFold(n_splits=5, n_repeats=3, random_state=123), \n",
    "                refit      = True,\n",
    "                verbose    = 0,\n",
    "                return_train_score = True\n",
    "               )\n",
    "\n",
    "        grid.fit(X = self.X_train, y = self.Y_train)\n",
    "\n",
    "        # Resultados\n",
    "        # ==============================================================================\n",
    "        resultados = pd.DataFrame(grid.cv_results_)\n",
    "        resultados.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "            .drop(columns = 'params') \\\n",
    "            .sort_values('mean_test_score', ascending = False) \\\n",
    "            .head(4)\n",
    "        \n",
    "        # Mejores hiperparámetros por validación cruzada\n",
    "        # ==============================================================================\n",
    "        print(\"----------------------------------------\")\n",
    "        print(\"Mejores hiperparámetros encontrados (cv)\")\n",
    "        print(\"----------------------------------------\")\n",
    "        print(grid.best_params_, \":\", grid.best_score_, grid.scoring)\n",
    "        \n",
    "        self.modelo_final = grid.best_estimator_\n",
    "        \n",
    "        '''\n",
    "        self.forest = RandomForestClassifier() \n",
    "        self.forest = self.forest.fit(self.X_train, self.Y_train)\n",
    "\n",
    "        predictions = self.forest.predict(self.X_cv) \n",
    "        self.Y_cv=list(self.Y_cv)\n",
    "        print(\"Accuracy: \", accuracy_score(self.Y_cv, predictions))\n",
    "        '''\n",
    "        \n",
    "        \n",
    "    def predecir_RF(self,txt):\n",
    "        \n",
    "        self.pred = self.vectorizer.transform(txt)\n",
    "        self.pred = self.pred.toarray()\n",
    "        predictions = self.forest.predict(self.pred) \n",
    "        #print(\"resultado: \" , predictions)\n",
    "        return list(predictions)\n",
    "    def predecir_Carpeta(self,txt):\n",
    "        \n",
    "        p=ProcesarDocumentos()\n",
    "        carpeta=p.resultadoStringCarpeta(txt)\n",
    "\n",
    "        resultados=[]\n",
    "        for i in range(len(carpeta)):\n",
    "            text=p.tratamientoTextos(carpeta[i])\n",
    "            hey=[\" \".join(text)]\n",
    "            resultados.append(self.predecir_RF(hey))\n",
    "        #print(\"Resultados: {}\".format(resultados))\n",
    "        \n",
    "        resultados=[]\n",
    "        for i in range(len(carpeta)):\n",
    "            text=p.tratamientoTextos(carpeta[i])\n",
    "            hey=\" \".join(text)\n",
    "            resultados.append(hey)\n",
    "        self.pred1 = self.vectorizer.transform(resultados)\n",
    "        self.pred1 = self.pred1.toarray()\n",
    "        predictions = self.forest.predict(self.pred1) \n",
    "        #print(\"resultado: \" , predictions)\n",
    "        return predictions\n",
    "        \n",
    "    def Entrenar_KNN(self):  \n",
    "        #self.preprocesamiento()\n",
    "        \n",
    "\n",
    "        vecinos = KNeighborsClassifier() \n",
    "        vecinos = vecinos.fit(self.X_train, self.Y_train)\n",
    "\n",
    "        predictions = vecinos.predict(self.X_cv) \n",
    "        self.Y_cv=list(self.Y_cv)\n",
    "        print(\"Accuracy: \", accuracy_score(self.Y_cv, predictions))\n",
    "\n",
    "    \n",
    "    def Entrenar_SVM(self):  \n",
    "        #self.preprocesamiento()\n",
    "        \n",
    "        #Create a svm Classifier\n",
    "        clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "        #Train the model using the training sets\n",
    "        clf.fit(self.X_train, self.Y_train)\n",
    "        \n",
    "\n",
    "        predictions = clf.predict(self.X_cv) \n",
    "        self.Y_cv=list(self.Y_cv)\n",
    "        print(\"Accuracy: \", accuracy_score(self.Y_cv, predictions))\n",
    "        \n",
    "        \n",
    "        cv=cross_val_score(clf, self.X_train, self.Y_train, cv=10)\n",
    "        \n",
    "        print(\"CV -> {}\".format(cv))\n",
    "        \n",
    "    def Entrenar_SVM_CV(self):\n",
    "        \n",
    "        \n",
    "        Cs = np.logspace(-6, -1, 10)\n",
    "        svc = svm.SVC()\n",
    "        clf = GridSearchCV(estimator=svc, param_grid=dict(C=Cs),\n",
    "                           n_jobs=-1)\n",
    "        clf.fit(self.X_train, self.Y_train)        \n",
    "\n",
    "        print(\"best score-> {}\".format(clf.best_score_))                                 \n",
    "\n",
    "        print(\"best estimator-> {}\".format(clf.best_estimator_.C))                            \n",
    "\n",
    "\n",
    "        # Prediction performance on test set is not as good as on train set\n",
    "        print(\"mi score\".format(clf.score(self.X_cv, self.Y_cv)))      \n",
    "\n",
    "    def Entrenar_Bayes(self):  \n",
    "        \n",
    "        #Create a svm Classifier\n",
    "        gaus = GaussianNB() # Linear Kernel\n",
    "\n",
    "        #Train the model using the training sets\n",
    "        gaus.fit(self.X_train, self.Y_train)\n",
    "        \n",
    "\n",
    "        predictions = gaus.predict(self.X_cv) \n",
    "        self.Y_cv=list(self.Y_cv)\n",
    "        print(\"Accuracy: \", accuracy_score(self.Y_cv, predictions))\n",
    "        \n",
    "        \n",
    "        cv=cross_val_score(gaus, self.X_train, self.Y_train, cv=10)\n",
    "        \n",
    "        print(\"CV -> {}\".format(cv))   \n",
    "        \n",
    "    \n",
    "    def regresionMultinomial(self):\n",
    "        \n",
    "        model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "        model.fit(self.X_train, self.Y_train)\n",
    "        \n",
    "\n",
    "        predictions = model.predict(self.X_cv) \n",
    "        self.Y_cv=list(self.Y_cv)\n",
    "        print(\"Accuracy: \", accuracy_score(self.Y_cv, predictions))\n",
    "        \n",
    "        \n",
    "        cv=cross_val_score(model, self.X_train, self.Y_train, cv=10)\n",
    "        \n",
    "        print(\"CV -> {}\".format(cv)) \n",
    "    def Entrenar_RedNeuronal(self):\n",
    "        \n",
    "        xtrain=[]\n",
    "        for i in range(len(self.X_train)):     \n",
    "            xtrain.append(list(self.X_train[i]))\n",
    "        #xtrain=np.array(xtrain)\n",
    "\n",
    "        xcv=[]\n",
    "        for i in range(len(self.X_cv)):     \n",
    "            xcv.append(list(self.X_cv[i]))\n",
    "        #xcv=np.array(xcv)\n",
    "        #Y_train=np.array(Y_train)\n",
    "        #Y_cv=np.array(Y_cv)\n",
    "\n",
    "\n",
    "        print(type(xtrain))\n",
    "        print(type(self.Y_train))\n",
    "        print(type(xcv))\n",
    "        print(type(self.Y_cv))\n",
    "        \n",
    "        \n",
    "        clear_session()\n",
    "        \n",
    "\n",
    "        #input_dim = xtrain.shape[1] #.shape[0]  # Number of features\n",
    "        input_dim= len(xtrain[0])\n",
    "        model = Sequential()\n",
    "        model.add(layers.Dense(7, input_dim=input_dim, activation='relu'))\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        '''\n",
    "        model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['binary_accuracy'])\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        loss_fn = tf.keras.losses.MeanSquaredError(reduction='sum_over_batch_size')\n",
    "        model.compile(loss=loss_fn, \n",
    "                       optimizer='adam', \n",
    "                       metrics=['accuracy'])\n",
    "        \n",
    "        print(model.summary())\n",
    "\n",
    "        xtrain2=[]\n",
    "        for i in range(len(xtrain)):\n",
    "            temp=[]\n",
    "            for j in range(len(xtrain[i])):\n",
    "                temp.append(int(xtrain[i][j]))\n",
    "            xtrain2.append(temp)\n",
    "        \n",
    "        xcv2=[]\n",
    "        for i in range(len(xcv)):\n",
    "            temp=[]\n",
    "            for j in range(len(xcv[i])):\n",
    "                temp.append(int(xcv[i][j]))\n",
    "            xcv2.append(temp)\n",
    "            \n",
    "        ytrain2=[]\n",
    "        for i in range(len(self.Y_train)):\n",
    "            ytrain2.append(int(self.Y_train[i]))\n",
    "            \n",
    "        ycv2=[]\n",
    "        for i in range(len(self.Y_cv)):\n",
    "            ycv2.append(int(self.Y_cv[i]))\n",
    "            \n",
    "        print(type(xtrain2[0][0]))    \n",
    "        self.xtrain2=xtrain2        \n",
    "        self.xtrain=xtrain\n",
    "        '''\n",
    "        history = model.fit(xtrain2, self.Y_train,\n",
    "                     epochs=10,\n",
    "                     verbose=False,\n",
    "                     validation_data=(xcv, self.Y_cv),\n",
    "                     batch_size=10)\n",
    "        '''\n",
    "        history = model.fit(xtrain2, ytrain2,\n",
    "                     epochs=10,\n",
    "                     verbose=False,\n",
    "                     validation_data=(xcv2, ycv2),\n",
    "                     batch_size=10)\n",
    "\n",
    "        \n",
    "        clear_session()\n",
    "        '''\n",
    "        loss, accuracy = model.evaluate(xtrain, self.Y_train, verbose=False)\n",
    "        print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "        loss, accuracy = model.evaluate(xcv, self.Y_cv, verbose=False)\n",
    "        print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "        '''\n",
    "        loss, accuracy = model.evaluate(xtrain2, ytrain2, verbose=False)\n",
    "        print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "        loss, accuracy = model.evaluate(xcv2, ycv2, verbose=False)\n",
    "        print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "\n",
    "    #return X_train, Y_train , X_cv , Y_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165b9ea9",
   "metadata": {},
   "source": [
    "## Probando modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96b2262",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1=modelos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff84345",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1.regresionMultinomial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b649ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1.Entrenar_Bayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe98e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1.Entrenar_SVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40c644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1.Entrenar_RF_CV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf5cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1.Entrenar_RF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8e16c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1.Entrenar_KNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6078296",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1.modelo_final.predict(mp1.X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d6af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "hey=[\" \".join(df[df['clasif']==0]['receta'].iloc[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b869864",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1.predecir_Carpeta(\"./Interfaz/testing/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2b0f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bca58dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelosTFIDF:\n",
    "    def __init__(self):\n",
    "        self.tfidf()\n",
    "    \n",
    "    def tfidf(self):\n",
    "        hola=[]\n",
    "        for i,j in enumerate(df['receta']):\n",
    "            hola.append(\" \".join(j))\n",
    "        self.vectorizers= TfidfVectorizer(max_features=4000)    \n",
    "        self.vect = self.vectorizers.fit_transform(hola)\n",
    "        arr=self.vect.toarray()\n",
    "        variable=self.vectorizers.get_feature_names()\n",
    "        \n",
    "\n",
    "        variables=dict.fromkeys(variable,None)\n",
    "\n",
    "        tf1=pd.DataFrame(variables,index=[0])\n",
    "        for i in range(len(df['clasif'])):\n",
    "            tf1.loc[i]=arr[i]\n",
    "        \n",
    "        self.X_train, self.X_cv, self.Y_train, self.Y_cv = train_test_split(tf1, df['clasif'], test_size = 0.2, random_state=42)\n",
    "        self.Y_train=list(self.Y_train)\n",
    "        self.Y_cv=list(self.Y_cv)\n",
    "         \n",
    "  \n",
    "        \n",
    "    def Entrenar_RF(self):\n",
    "        # Grid de hiperparámetros evaluados\n",
    "        # ==============================================================================\n",
    "        \n",
    "        print(self.X_train.shape)\n",
    "        param_grid = ParameterGrid(\n",
    "                        {'n_estimators': [1000],\n",
    "                         'max_features': [5, 7, 9],\n",
    "                         'max_depth'   : [None, 3, 10, 20],\n",
    "                         'criterion'   : ['gini', 'entropy']\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        # Loop para ajustar un modelo con cada combinación de hiperparámetros\n",
    "        # ==============================================================================\n",
    "        resultados = {'params': [], 'oob_accuracy': []}\n",
    "\n",
    "        for params in param_grid:\n",
    "\n",
    "            modelo = RandomForestClassifier(\n",
    "                        oob_score    = True,\n",
    "                        n_jobs       = -1,\n",
    "                        random_state = 123,\n",
    "                        ** params\n",
    "                     )\n",
    "\n",
    "            modelo.fit(self.X_train, self.Y_train)\n",
    "\n",
    "            resultados['params'].append(params)\n",
    "            resultados['oob_accuracy'].append(modelo.oob_score_)\n",
    "            print(f\"Modelo: {params} \\u2713\")\n",
    "\n",
    "        # Resultados\n",
    "        # ==============================================================================\n",
    "        resultados = pd.DataFrame(resultados)\n",
    "        resultados = pd.concat([resultados, resultados['params'].apply(pd.Series)], axis=1)\n",
    "        resultados = resultados.sort_values('oob_accuracy', ascending=False)\n",
    "        resultados = resultados.drop(columns = 'params')\n",
    "        print(resultados.head(4))\n",
    "        \n",
    "        '''\n",
    "        self.forest = RandomForestClassifier() \n",
    "        self.forest = self.forest.fit(self.X_train, self.Y_train)\n",
    "\n",
    "        predictions = self.forest.predict(self.X_cv) \n",
    "        self.Y_cv=list(self.Y_cv)\n",
    "        print(\"Accuracy: \", accuracy_score(self.Y_cv, predictions))\n",
    "        '''\n",
    "        \n",
    "    def Entrenar_RF_CV(self):\n",
    "        # Grid de hiperparámetros evaluados\n",
    "        # ==============================================================================\n",
    "        param_grid = {'n_estimators': [150],\n",
    "                      'max_features': [5, 7, 9],\n",
    "                      'max_depth'   : [None, 3, 10, 20],\n",
    "                      'criterion'   : ['gini', 'entropy']\n",
    "                     }\n",
    "\n",
    "        # Búsqueda por grid search con validación cruzada\n",
    "        # ==============================================================================\n",
    "        \n",
    "        grid = GridSearchCV(\n",
    "                estimator  = RandomForestClassifier(random_state = 123),\n",
    "                param_grid = param_grid,\n",
    "                scoring    = 'accuracy',\n",
    "                n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "                cv         = RepeatedKFold(n_splits=5, n_repeats=3, random_state=123), \n",
    "                refit      = True,\n",
    "                verbose    = 0,\n",
    "                return_train_score = True\n",
    "               )\n",
    "\n",
    "        grid.fit(X = self.X_train, y = self.Y_train)\n",
    "\n",
    "        # Resultados\n",
    "        # ==============================================================================\n",
    "        resultados = pd.DataFrame(grid.cv_results_)\n",
    "        resultados.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "            .drop(columns = 'params') \\\n",
    "            .sort_values('mean_test_score', ascending = False) \\\n",
    "            .head(4)\n",
    "        \n",
    "        # Mejores hiperparámetros por validación cruzada\n",
    "        # ==============================================================================\n",
    "        print(\"----------------------------------------\")\n",
    "        print(\"Mejores hiperparámetros encontrados (cv)\")\n",
    "        print(\"----------------------------------------\")\n",
    "        print(grid.best_params_, \":\", grid.best_score_, grid.scoring)\n",
    "        \n",
    "        self.modelo_final = grid.best_estimator_\n",
    "        \n",
    "        '''\n",
    "        self.forest = RandomForestClassifier() \n",
    "        self.forest = self.forest.fit(self.X_train, self.Y_train)\n",
    "\n",
    "        predictions = self.forest.predict(self.X_cv) \n",
    "        self.Y_cv=list(self.Y_cv)\n",
    "        print(\"Accuracy: \", accuracy_score(self.Y_cv, predictions))\n",
    "        '''\n",
    "\n",
    "    def Entrenar_SVM(self):  \n",
    "        #self.preprocesamiento()\n",
    "        \n",
    "        #Create a svm Classifier\n",
    "        m_SVM = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "        #Train the model using the training sets\n",
    "        m_SVM.fit(self.X_train, self.Y_train)\n",
    "        \n",
    "\n",
    "        predictions = m_SVM.predict(self.X_cv) \n",
    "        self.Y_cv=list(self.Y_cv)\n",
    "        print(\"Accuracy: \", accuracy_score(self.Y_cv, predictions))\n",
    "        \n",
    "        \n",
    "        cv=cross_val_score(m_SVM, self.X_train, self.Y_train, cv=10)\n",
    "        self.m_SVM=m_SVM\n",
    "        \n",
    "        print(\"CV -> {}\".format(cv))\n",
    "        return self.m_SVM\n",
    "        \n",
    "    def Entrenar_SVM_CV(self):\n",
    "        \n",
    "        Cs = np.logspace(-6, -1, 10)\n",
    "        svc = svm.SVC()\n",
    "        m_SVM_CV = GridSearchCV(estimator=svc, param_grid=dict(C=Cs),\n",
    "                           n_jobs=-1)\n",
    "        m_SVM_CV.fit(self.X_train, self.Y_train)        \n",
    "\n",
    "        print(\"best score-> {}\".format(m_SVM_CV.best_score_))                                 \n",
    "\n",
    "        print(\"best estimator-> {}\".format(m_SVM_CV.best_estimator_.C))                            \n",
    "\n",
    "\n",
    "        # Prediction performance on test set is not as good as on train set\n",
    "        print(\"mi score\".format(m_SVM_CV.score(self.X_cv, self.Y_cv)))      \n",
    "\n",
    "    def Entrenar_Bayes(self):\n",
    "        \n",
    "        \n",
    "        #Create a svm Classifier\n",
    "        gaus = GaussianNB() # Linear Kernel\n",
    "\n",
    "        #Train the model using the training sets\n",
    "        gaus.fit(self.X_train, self.Y_train)\n",
    "        \n",
    "\n",
    "        predictions = gaus.predict(self.X_cv) \n",
    "        self.Y_cv=list(self.Y_cv)\n",
    "        print(\"Accuracy: \", accuracy_score(self.Y_cv, predictions))\n",
    "        \n",
    "        \n",
    "        cv=cross_val_score(gaus, self.X_train, self.Y_train, cv=10)\n",
    "        self.gaus=gaus\n",
    "        print(\"CV -> {}\".format(cv))   \n",
    "        \n",
    "    \n",
    "    def regresionMultinomial(self):\n",
    "        \n",
    "        M_mult = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "        M_mult.fit(self.X_train, self.Y_train)\n",
    "        \n",
    "        predictions = M_mult.predict(self.X_cv) \n",
    "        self.Y_cv=list(self.Y_cv)\n",
    "        print(\"Accuracy: \", accuracy_score(self.Y_cv, predictions))\n",
    "        \n",
    "        #cv=cross_val_score(M_mult, self.X_train, self.Y_train, cv=10)\n",
    "        self.M_mult=M_mult\n",
    "        #print(\"CV -> {}\".format(cv)) \n",
    "    \n",
    "    def predecir_RF(self,txt):\n",
    "        \n",
    "        self.pred = self.vectorizers.transform(txt)\n",
    "        self.pred = self.pred.toarray()\n",
    "        predictions = self.M_mult.predict(self.pred) \n",
    "        print(\"resultado: \" , predictions)\n",
    "        \n",
    "    def clasificar(self,modelo,txt):\n",
    "        \n",
    "        self.pred = self.vectorizers.transform(txt)\n",
    "        self.pred = self.pred.toarray()\n",
    "        predictions = modelo.predict(self.pred) \n",
    "        print(\"resultado: \" , predictions)\n",
    "    \n",
    "    def predecir_Carpeta(self,txt):\n",
    "        \n",
    "        p=ProcesarDocumentos()\n",
    "        carpeta=p.resultadoStringCarpeta(txt)\n",
    "\n",
    "        resultados=[]\n",
    "        for i in range(len(carpeta)):\n",
    "            text=p.tratamientoTextos(carpeta[i])\n",
    "            hey=[\" \".join(text)]\n",
    "            resultados.append(self.predecir_RF(hey))\n",
    "        #print(\"Resultados: {}\".format(resultados))\n",
    "        \n",
    "        resultados=[]\n",
    "        for i in range(len(carpeta)):\n",
    "            text=p.tratamientoTextos(carpeta[i])\n",
    "            hey=\" \".join(text)\n",
    "            resultados.append(hey)\n",
    "        self.pred1 = self.vectorizers.transform(resultados)\n",
    "        self.pred1 = self.pred1.toarray()\n",
    "        predictions = self.M_mult.predict(self.pred1) \n",
    "        #print(\"resultado: \" , predictions)\n",
    "        return predictions\n",
    "        \n",
    "    def Entrenar_KNN(self):  \n",
    "        #self.preprocesamiento()\n",
    "        \n",
    "\n",
    "        vecinos = KNeighborsClassifier() \n",
    "        vecinos = vecinos.fit(self.X_train, self.Y_train)\n",
    "\n",
    "        predictions = vecinos.predict(self.X_cv) \n",
    "        self.Y_cv=list(self.Y_cv)\n",
    "        print(\"Accuracy: \", accuracy_score(self.Y_cv, predictions))\n",
    "        \n",
    "    \n",
    "    def Entrenar_RedNeuronal(self):\n",
    "        xtrain=[]\n",
    "        for i in range(len(self.X_train)):     \n",
    "            xtrain.append(list(self.X_train.iloc[i]))\n",
    "        #xtrain=np.array(xtrain)\n",
    "\n",
    "        xcv=[]\n",
    "        for i in range(len(self.X_cv)):     \n",
    "            xcv.append(list(self.X_cv.iloc[i]))\n",
    "        #xcv=np.array(xcv)\n",
    "        #Y_train=np.array(Y_train)\n",
    "        #Y_cv=np.array(Y_cv)\n",
    "\n",
    "\n",
    "        print(type(xtrain))\n",
    "        print(type(self.Y_train))\n",
    "        print(type(xcv))\n",
    "        print(type(self.Y_cv))\n",
    "        \n",
    "        \n",
    "        clear_session()\n",
    "        \n",
    "\n",
    "        #input_dim = xtrain.shape[1] #.shape[0]  # Number of features\n",
    "        input_dim= len(xtrain[0])\n",
    "        model = Sequential()\n",
    "        model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "        model.add(layers.Dense(80, input_dim=100, activation='relu'))\n",
    "        model.add(layers.Dense(20, input_dim=80, activation='relu'))\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "        print(model.summary())\n",
    "        history = model.fit(xtrain, self.Y_train,\n",
    "                     epochs=1000,\n",
    "                     verbose=False,\n",
    "                     validation_data=(xcv, self.Y_cv),\n",
    "                     batch_size=10)\n",
    "\n",
    "        \n",
    "        clear_session()\n",
    "\n",
    "        loss, accuracy = model.evaluate(xtrain, self.Y_train, verbose=False)\n",
    "        print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "        loss, accuracy = model.evaluate(xcv, self.Y_cv, verbose=False)\n",
    "        print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "\n",
    "    def guardarModelo(self,modelo,nombre):\n",
    "        \n",
    "        joblib.dump(modelo, './Interfaz/modelos/{}.pkl'.format(nombre)) # Guardo el modelo.\n",
    "    \n",
    "    def cargarModelo(self,nombre):\n",
    "        \n",
    "        return joblib.load('./Interfaz/modelos/{}.pkl'.format(nombre))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd31ea1c",
   "metadata": {},
   "source": [
    "## Probando modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bed0ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_tf=modelosTFIDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d11b6ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(650, 4000)\n",
      "Modelo: {'criterion': 'gini', 'max_depth': None, 'max_features': 5, 'n_estimators': 1000} ✓\n",
      "Modelo: {'criterion': 'gini', 'max_depth': None, 'max_features': 7, 'n_estimators': 1000} ✓\n",
      "Modelo: {'criterion': 'gini', 'max_depth': None, 'max_features': 9, 'n_estimators': 1000} ✓\n",
      "Modelo: {'criterion': 'gini', 'max_depth': 3, 'max_features': 5, 'n_estimators': 1000} ✓\n",
      "Modelo: {'criterion': 'gini', 'max_depth': 3, 'max_features': 7, 'n_estimators': 1000} ✓\n",
      "Modelo: {'criterion': 'gini', 'max_depth': 3, 'max_features': 9, 'n_estimators': 1000} ✓\n",
      "Modelo: {'criterion': 'gini', 'max_depth': 10, 'max_features': 5, 'n_estimators': 1000} ✓\n",
      "Modelo: {'criterion': 'gini', 'max_depth': 10, 'max_features': 7, 'n_estimators': 1000} ✓\n",
      "Modelo: {'criterion': 'gini', 'max_depth': 10, 'max_features': 9, 'n_estimators': 1000} ✓\n",
      "Modelo: {'criterion': 'gini', 'max_depth': 20, 'max_features': 5, 'n_estimators': 1000} ✓\n",
      "Modelo: {'criterion': 'gini', 'max_depth': 20, 'max_features': 7, 'n_estimators': 1000} ✓\n",
      "Modelo: {'criterion': 'gini', 'max_depth': 20, 'max_features': 9, 'n_estimators': 1000} ✓\n",
      "Modelo: {'criterion': 'entropy', 'max_depth': None, 'max_features': 5, 'n_estimators': 1000} ✓\n",
      "Modelo: {'criterion': 'entropy', 'max_depth': None, 'max_features': 7, 'n_estimators': 1000} ✓\n",
      "Modelo: {'criterion': 'entropy', 'max_depth': None, 'max_features': 9, 'n_estimators': 1000} ✓\n",
      "Modelo: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 5, 'n_estimators': 1000} ✓\n",
      "Modelo: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 7, 'n_estimators': 1000} ✓\n",
      "Modelo: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 9, 'n_estimators': 1000} ✓\n",
      "Modelo: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 5, 'n_estimators': 1000} ✓\n",
      "Modelo: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 7, 'n_estimators': 1000} ✓\n",
      "Modelo: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 9, 'n_estimators': 1000} ✓\n",
      "Modelo: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 5, 'n_estimators': 1000} ✓\n",
      "Modelo: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 7, 'n_estimators': 1000} ✓\n",
      "Modelo: {'criterion': 'entropy', 'max_depth': 20, 'max_features': 9, 'n_estimators': 1000} ✓\n",
      "    oob_accuracy criterion  max_depth  max_features  n_estimators\n",
      "2       0.723077      gini        NaN             9          1000\n",
      "1       0.723077      gini        NaN             7          1000\n",
      "0       0.713846      gini        NaN             5          1000\n",
      "14      0.709231   entropy        NaN             9          1000\n"
     ]
    }
   ],
   "source": [
    "m_tf.Entrenar_RF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "907c2587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8343558282208589\n",
      "CV -> [0.78461538 0.86153846 0.8        0.83076923 0.90769231 0.92307692\n",
      " 0.84615385 0.83076923 0.90769231 0.76923077]\n"
     ]
    }
   ],
   "source": [
    "modelo1=m_tf.Entrenar_SVM()  #entrenamos y guardamos el modelo en una variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1eea11cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'break_ties': False,\n",
       " 'cache_size': 200,\n",
       " 'class_weight': None,\n",
       " 'coef0': 0.0,\n",
       " 'decision_function_shape': 'ovr',\n",
       " 'degree': 3,\n",
       " 'gamma': 'scale',\n",
       " 'kernel': 'linear',\n",
       " 'max_iter': -1,\n",
       " 'probability': False,\n",
       " 'random_state': None,\n",
       " 'shrinking': True,\n",
       " 'tol': 0.001,\n",
       " 'verbose': False}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo1.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdf3256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hola=modelo1.n_features_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a6ce9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3900"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hola-100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cc6529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_tf.guardarModelo(modelo1,\"modelo1_SVM\")  #guardamos el modelo en formato plk en la dirección -> ./Interfaz/modelos/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d9166e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeloImportado=m_tf.cargarModelo(\"modelo1_SVM\")  #cargamos un modelo externo y lo guardamos en una variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "393b2a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeloImportado.n_features_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeb3bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hey=[\" \".join(df['receta'][400])]\n",
    "m_tf.clasificar(modelo1,hey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed28c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_tf.Entrenar_Bayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137ef526",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_tf.regresionMultinomial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476b8091",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_tf.predecir_Carpeta(\"./Interfaz/Carpeta Testing/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e7f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hola(hey):\n",
    "    string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bec36eb",
   "metadata": {},
   "source": [
    "### TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5eaf48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b982ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hola=[]\n",
    "for i,j in enumerate(df['receta']):\n",
    "    hola.append(\" \".join(j))\n",
    "vectorizer= TfidfVectorizer()    \n",
    "vect = vectorizer.fit_transform(hola)\n",
    "\n",
    "variable=vectorizer.get_feature_names()\n",
    "arr=vect.toarray()\n",
    "v=[]\n",
    "for i in df['clasif'].unique():\n",
    "    if i==0:\n",
    "        v.append(arr[:len(df[df['clasif']==i])])\n",
    "    elif i<max(df['clasif'].unique()):\n",
    "        v.append(arr[len(df[df['clasif']<=i-1]):len(df[df['clasif']<=i])])\n",
    "    else:\n",
    "        v.append(arr[len(df[df['clasif']==i-1]):])\n",
    "\n",
    "w=[]\n",
    "for i in range(len(v)):\n",
    "    w.append(sum(v[i]))\n",
    "    \n",
    "\n",
    "\n",
    "variables=dict.fromkeys(variable,None)\n",
    "    \n",
    "tf1=pd.DataFrame(variables,index=[0])\n",
    "for i in df['clasif'].unique():\n",
    "    tf1.loc[i]=w[i]\n",
    "\n",
    "tf1.index = pd.Series(['Arroz', 'Bebidas', 'Carnes','Marisco','Pasta','Pescados','PlatosMenores','Verduras'])\n",
    "tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03713614",
   "metadata": {},
   "outputs": [],
   "source": [
    "hola=[]\n",
    "for i,j in enumerate(df['receta']):\n",
    "    hola.append(\" \".join(j))\n",
    "vectorizers= TfidfVectorizer(max_features=4000)    \n",
    "vect = vectorizers.fit_transform(hola)\n",
    "\n",
    "variable=vectorizers.get_feature_names()\n",
    "arr=vect.toarray()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "variables=dict.fromkeys(variable,None)\n",
    "\n",
    "tf1=pd.DataFrame(variables,index=[0])\n",
    "for i in range(len(df['clasif'])):\n",
    "    tf1.loc[i]=arr[i]\n",
    "\n",
    "tf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce6f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_cv, Y_train, Y_cv = train_test_split(tf1, df['clasif'], test_size = 0.3, random_state=42)\n",
    "Y_train=list(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac47c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier() \n",
    "forest = forest.fit(X_train, Y_train)\n",
    "\n",
    "predictions = forest.predict(X_cv) \n",
    "Y_cv=list(Y_cv)\n",
    "print(\"Accuracy: \", accuracy_score(Y_cv, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce96366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term(dict, search_index):\n",
    "    return list(dict.keys())[list(dict.values()).index(search_index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3ac10a",
   "metadata": {},
   "source": [
    "### Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ad5646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4de485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamiento de datos\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# Gráficos\n",
    "# ==============================================================================\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Modelado\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# Configuración warnings\n",
    "# ==============================================================================\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e21e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_cv, Y_train, Y_cv = train_test_split(tf1, df['clasif'], test_size = 0.2, random_state=42)\n",
    "Y_train=list(Y_train)\n",
    "Y_cv=list(Y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9926ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b5947",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565e3374",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ytrain=np.array(Y_train)\n",
    "ytest=np.array(Y_cv)\n",
    "ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3dead0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71e6861",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain=[]\n",
    "for i in range(len(X_train)):     \n",
    "    xtrain.append(list(X_train.iloc[i]))\n",
    "#xtrain=np.array(xtrain)\n",
    "\n",
    "xcv=[]\n",
    "for i in range(len(X_cv)):     \n",
    "    xcv.append(list(X_cv.iloc[i]))\n",
    "#xcv=np.array(xcv)\n",
    "#Y_train=np.array(Y_train)\n",
    "#Y_cv=np.array(Y_cv)\n",
    "\n",
    "\n",
    "print(type(xtrain))\n",
    "print(type(Y_train))\n",
    "print(type(xcv))\n",
    "print(type(Y_cv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ce433",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(m.xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076d6852",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a574cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clear_session()\n",
    "\n",
    "\n",
    "#input_dim = xtrain.shape[1] #.shape[0]  # Number of features\n",
    "input_dim= len(xtrain[0])\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(6000, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0bf33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceab2968",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "loss_fn = tf.keras.losses.MeanSquaredError(reduction='sum_over_batch_size')\n",
    "model.compile(loss=loss_fn, \n",
    "               optimizer='adam', \n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f15d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbceec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7ab43d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()\n",
    "history = model.fit(xtrain, Y_train,\n",
    "                     epochs=2000,\n",
    "                     verbose=False,\n",
    "                     validation_data=(xcv, Y_cv),\n",
    "                     batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592419ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbaaf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(xtrain, Y_train,\n",
    "                     epochs=2000,\n",
    "                     verbose=False,\n",
    "                     validation_data=(xcv, Y_cv),\n",
    "                     batch_size=10)\n",
    "\n",
    "\n",
    "clear_session()\n",
    "\n",
    "loss, accuracy = model.evaluate(xtrain, Y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(xcv, Y_cv, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8a462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(xtrain)):\n",
    "    if i<len(xtrain)-1:\n",
    "        if len(xtrain[i])!=len(xtrain[i+1]):\n",
    "            print(\"hay algo mal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5d0b78",
   "metadata": {},
   "source": [
    "### Prueba Red Neuronal manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069f927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class capa():\n",
    "    def __init__(self, n_neuronas_capa_anterior, n_neuronas, funcion_act):\n",
    "        self.funcion_act = funcion_act\n",
    "        self.b  = np.round(stats.truncnorm.rvs(-1, 1, loc=0, scale=1, size= n_neuronas).reshape(1,n_neuronas),3)\n",
    "        self.W  = np.round(stats.truncnorm.rvs(-1, 1, loc=0, scale=1, size= n_neuronas * n_neuronas_capa_anterior).reshape(n_neuronas_capa_anterior,n_neuronas),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "sigmoid = (\n",
    "  lambda x:1 / (1 + np.exp(-x)),\n",
    "  lambda x:x * (1 - x)\n",
    "  )\n",
    "\n",
    "rango = np.linspace(-10,10).reshape([50,1])\n",
    "datos_sigmoide = sigmoid[0](rango)\n",
    "datos_sigmoide_derivada = sigmoid[1](rango)\n",
    "\n",
    "#Cremos los graficos\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize =(15,5))\n",
    "axes[0].plot(rango, datos_sigmoide)\n",
    "axes[1].plot(rango, datos_sigmoide_derivada)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c67cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivada_relu(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "relu = (\n",
    "  lambda x: x * (x > 0),\n",
    "  lambda x:derivada_relu(x)\n",
    "  )\n",
    "\n",
    "datos_relu = relu[0](rango)\n",
    "datos_relu_derivada = relu[1](rango)\n",
    "\n",
    "\n",
    "# Volvemos a definir rango que ha sido cambiado\n",
    "rango = np.linspace(-10,10).reshape([50,1])\n",
    "\n",
    "# Cremos los graficos\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize =(15,5))\n",
    "axes[0].plot(rango, datos_relu[:,0])\n",
    "axes[1].plot(rango, datos_relu_derivada[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc237fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numero de neuronas en cada capa. \n",
    "# El primer valor es el numero de columnas de la capa de entrada.\n",
    "neuronas = [2,4,8,1] \n",
    "\n",
    "# Funciones de activacion usadas en cada capa. \n",
    "funciones_activacion = [relu,relu, sigmoid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2126e41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_neuronal = []\n",
    "\n",
    "for paso in range(len(neuronas)-1):   \n",
    "    x = capa(neuronas[paso],neuronas[paso+1],funciones_activacion[paso])\n",
    "    red_neuronal.append(x)\n",
    "\n",
    "print(red_neuronal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c438548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_neuronal[0].W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac8882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X =  np.round(np.random.randn(20,2),3) # Ejemplo de vector de entrada\n",
    "\n",
    "z = X @ red_neuronal[0].W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef17c6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = z + red_neuronal[0].b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431fb687",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = red_neuronal[0].funcion_act[0](z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd0b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = [X]\n",
    "\n",
    "for num_capa in range(len(red_neuronal)):\n",
    "    z = output[-1] @ red_neuronal[num_capa].W + red_neuronal[num_capa].b\n",
    "    a = red_neuronal[num_capa].funcion_act[0](z)\n",
    "    output.append(a)\n",
    "\n",
    "print(output[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d4ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(Ypredich, Yreal):    \n",
    " # Calculamos el error\n",
    "    x = (np.array(Ypredich) - np.array(Yreal)) ** 2\n",
    "    x = np.mean(x)\n",
    " # Calculamos la derivada de la funcion\n",
    "    y = np.array(Ypredich) - np.array(Yreal)\n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a03c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Y = [0] * 10 + [1] * 10\n",
    "shuffle(Y)\n",
    "Y = np.array(Y).reshape(len(Y),1)\n",
    "\n",
    "mse(output[-1], Y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305d382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprop en la ultima capa\n",
    "a = output[-1]\n",
    "x = mse(a,Y)[1] * red_neuronal[-2].funcion_act[1](a)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f438772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_neuronal[-1].b = red_neuronal[-1].b - x.mean() * 0.01\n",
    "red_neuronal[-1].W = red_neuronal[-1].W - (output[-1].T @ x) * 0.01\n",
    "\n",
    "red_neuronal[-1].b\n",
    "red_neuronal[-1].W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca537f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el learning rate\n",
    "lr = 0.05\n",
    "\n",
    "# Creamos el indice inverso para ir de derecha a izquierda\n",
    "back = list(range(len(output)-1))\n",
    "back.reverse()\n",
    "\n",
    "# Creamos el vector delta donde meteremos los errores en cada capa\n",
    "delta = []\n",
    "\n",
    "for capa in back:\n",
    "  # Backprop #\n",
    "\n",
    "  # Guardamos los resultados de la ultima capa antes de usar backprop para poder usarlas en gradient descent\n",
    "    a = output[capa+1][1]\n",
    "\n",
    "  # Backprop en la ultima capa \n",
    "    if capa == back[0]:      \n",
    "        x = mse(a,Y)[1] * red_neuronal[capa].funcion_act[1](a)\n",
    "        delta.append(x)\n",
    "\n",
    "  # Backprop en el resto de capas \n",
    "    else:\n",
    "        x = delta[-1] @ W_temp * red_neuronal[capa].funcion_act[1](a)\n",
    "        delta.append(x)\n",
    "\n",
    "  # Guardamos los valores de W para poder usarlos en la iteracion siguiente\n",
    "    W_temp = red_neuronal[capa].W.transpose()\n",
    "\n",
    "  # Gradient Descent #\n",
    "\n",
    "  # Ajustamos los valores de los parametros de la capa\n",
    "    red_neuronal[capa].b = red_neuronal[capa].b - delta[-1].mean() * lr\n",
    "    red_neuronal[capa].W = red_neuronal[capa].W - (output[capa].T @ delta[-1]) * lr\n",
    "\n",
    "\n",
    "print('MSE: ' + str(mse(output[-1],Y)[0]) )\n",
    "print('Estimacion: ' + str(output[-1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f716186",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def circulo(num_datos = 100,R = 1, minimo = 0,maximo= 1):\n",
    "    pi = math.pi\n",
    "    r = R * np.sqrt(stats.truncnorm.rvs(minimo, maximo, size= num_datos)) * 10\n",
    "    theta = stats.truncnorm.rvs(minimo, maximo, size= num_datos) * 2 * pi *10\n",
    "\n",
    "    x = np.cos(theta) * r\n",
    "    y = np.sin(theta) * r\n",
    "\n",
    "    y = y.reshape((num_datos,1))\n",
    "    x = x.reshape((num_datos,1))\n",
    "\n",
    "  #Vamos a reducir el numero de elementos para que no cause un Overflow\n",
    "    x = np.round(x,3)\n",
    "    y = np.round(y,3)\n",
    "\n",
    "    df = np.column_stack([x,y])\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffeb7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_1 = circulo(num_datos = 150, R = 2)\n",
    "datos_2 = circulo(num_datos = 150, R = 0.5)\n",
    "X = np.concatenate([datos_1,datos_2])\n",
    "X = np.round(X,3)\n",
    "\n",
    "Y = [0] * 150 + [1] * 150\n",
    "Y = np.array(Y).reshape(len(Y),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7406f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.cla()\n",
    "plt.scatter(X[0:150,0],X[0:150,1], c = \"b\")\n",
    "plt.scatter(X[150:300,0],X[150:300,1], c = \"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58a39d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenamiento(X,Y, red_neuronal, lr = 0.01):\n",
    "\n",
    "  # Output guardara el resultado de cada capa\n",
    "  # En la capa 1, el resultado es el valor de entrada\n",
    "    output = [X]\n",
    "\n",
    "    for num_capa in range(len(red_neuronal)):\n",
    "        \n",
    "        z = output[-1] @ red_neuronal[num_capa].W + red_neuronal[num_capa].b\n",
    "\n",
    "        a = red_neuronal[num_capa].funcion_act[0](z)\n",
    "\n",
    "    # Incluimos el resultado de la capa a output\n",
    "        output.append(a)\n",
    "\n",
    "  # Backpropagation\n",
    "\n",
    "    back = list(range(len(output)-1))\n",
    "    back.reverse()\n",
    "\n",
    "  # Guardaremos el error de la capa en delta  \n",
    "    delta = []\n",
    "\n",
    "    for capa in back:\n",
    "    # Backprop #delta\n",
    "\n",
    "        a = output[capa+1]\n",
    "\n",
    "        if capa == back[0]:\n",
    "            x = mse(a,Y)[1] * red_neuronal[capa].funcion_act[1](a)\n",
    "            delta.append(x)\n",
    "\n",
    "        else:\n",
    "            x = delta[-1] @ W_temp * red_neuronal[capa].funcion_act[1](a)\n",
    "            delta.append(x)\n",
    "\n",
    "        W_temp = red_neuronal[capa].W.transpose()\n",
    "\n",
    "    # Gradient Descent #\n",
    "        red_neuronal[capa].b = red_neuronal[capa].b - np.mean(delta[-1], axis = 0, keepdims = True) * lr\n",
    "        red_neuronal[capa].W = red_neuronal[capa].W - output[capa].transpose() @ delta[-1] * lr\n",
    "\n",
    "    return output[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5910605",
   "metadata": {},
   "outputs": [],
   "source": [
    "class capa():\n",
    "    \n",
    "    def __init__(self, n_neuronas_capa_anterior, n_neuronas, funcion_act):\n",
    "        \n",
    "        self.funcion_act = funcion_act\n",
    "        self.b  = np.round(stats.truncnorm.rvs(-1, 1, loc=0, scale=1, size= n_neuronas).reshape(1,n_neuronas),3)\n",
    "        self.W  = np.round(stats.truncnorm.rvs(-1, 1, loc=0, scale=1, size= n_neuronas * n_neuronas_capa_anterior).reshape(n_neuronas_capa_anterior,n_neuronas),3)\n",
    "\n",
    "neuronas = [2,4,8,1] \n",
    "funciones_activacion = [relu,relu, sigmoid]\n",
    "red_neuronal = []\n",
    "\n",
    "for paso in list(range(len(neuronas)-1)):\n",
    "    x = capa(neuronas[paso],neuronas[paso+1],funciones_activacion[paso])\n",
    "    red_neuronal.append(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed3094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = []\n",
    "predicciones = []\n",
    "\n",
    "for epoch in range(0,1000):\n",
    "    ronda = entrenamiento(X = X ,Y = Y ,red_neuronal = red_neuronal, lr = 0.001)\n",
    "    predicciones.append(ronda)\n",
    "    temp = mse(np.round(predicciones[-1]),Y)[0]\n",
    "    error.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a00f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = list(range(0,1000))\n",
    "plt.plot(epoch, error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d3b441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
